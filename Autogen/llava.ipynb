{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c62ad05-21ad-488d-a18b-29e288a510ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this variable to control where you want to host LLaVA, locally or remotely?\n",
    "# More details in the two setup options below.\n",
    "LLAVA_MODE = \"local\" # Either \"local\" or \"remote\"\n",
    "\n",
    "assert LLAVA_MODE in [\"local\", \"remote\"]\n",
    "\n",
    "import os\n",
    "## alternatively, you can put your API key here for the environment variable.\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_U1JTCjH9dvsk9IYWWnGeYLQ9jkHSr0c29XbZF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13271ce4-61d5-4324-9f56-beb5898f80d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the image, a green parrot is standing on a rocky surface, possibly a stone or a concrete slab. The parrot is not driving a car, and the scene does not involve any vehicles. The image is not similar to the text prompt of a parrot driving a car, as it only shows the parrot standing on a rocky surface. On a scale of 1 to 10, the similarity between the image and the text prompt is  1.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
    "import autogen\n",
    "import replicate\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import http.client\n",
    "import json\n",
    "import base64\n",
    "\n",
    "# config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n",
    "# llm_config = {\"config_list\": config_list, \"request_timeout\": 120}\n",
    "\n",
    "# function to use llava model to review image\n",
    "\n",
    "def img_review(image_path, prompt):\n",
    "    output = replicate.run(\n",
    "        \"yorickvp/llava-13b:6bc1c7bb0d2a34e413301fee8f7cc728d2d4e75bfab186aa995f63292bda92fc\",\n",
    "        input={\n",
    "            \"image\": open(image_path, \"rb\"),\n",
    "            \"prompt\": f\"What is happening in the image? From scale 1 to 10, decide how similar the image is to the text prompt {prompt}?\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for item in output:\n",
    "        result += item\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "result = img_review(\"/workspace/downloads/parrot/06qFTIVaZaY.jpg\", \"a parrot driving a car\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf2beae0-de4f-4567-9324-b4deebcd6108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'models': ['llava-v1.5-7b']}\n",
      "Model Name: llava-v1.5-7b\n"
     ]
    }
   ],
   "source": [
    "# Run this code block only if you want to run LlaVA locally\n",
    "import requests\n",
    "import json\n",
    "from llava.conversation import default_conversation as conv\n",
    "from llava.conversation import Conversation\n",
    "\n",
    "# Setup some global constants for convenience\n",
    "# Note: make sure the addresses below are consistent with your setup in LLaVA \n",
    "CONTROLLER_ADDR = \"http://0.0.0.0:10000\"\n",
    "SEP =  conv.sep\n",
    "ret = requests.post(CONTROLLER_ADDR + \"/list_models\")\n",
    "print(ret.json())\n",
    "MODEL_NAME = ret.json()[\"models\"][0]\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5c07a2-2b30-4d55-a153-40ee1fb681db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def get_image_data(image_file, use_b64=True):\n",
    "    if image_file.startswith('http://') or image_file.startswith('https://'):\n",
    "        response = requests.get(image_file)\n",
    "        content = response.content\n",
    "    elif re.match(r\"data:image/(?:png|jpeg);base64,\", image_file):\n",
    "        return re.sub(r\"data:image/(?:png|jpeg);base64,\", \"\", image_file)\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        content = buffered.getvalue()\n",
    "        \n",
    "    if use_b64:\n",
    "        return base64.b64encode(content).decode('utf-8')\n",
    "    else:\n",
    "        return content\n",
    "\n",
    "def lmm_formater(prompt: str, order_image_tokens: bool = False) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Formats the input prompt by replacing image tags and returns the new prompt along with image locations.\n",
    "    \n",
    "    Parameters:\n",
    "        - prompt (str): The input string that may contain image tags like <img ...>.\n",
    "        - order_image_tokens (bool, optional): Whether to order the image tokens with numbers. \n",
    "            It will be useful for GPT-4V. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "        - Tuple[str, List[str]]: A tuple containing the formatted string and a list of images (loaded in b64 format).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variables\n",
    "    new_prompt = prompt\n",
    "    image_locations = []\n",
    "    images = []\n",
    "    image_count = 0\n",
    "    \n",
    "    # Regular expression pattern for matching <img ...> tags\n",
    "    img_tag_pattern = re.compile(r'<img ([^>]+)>')\n",
    "    \n",
    "    # Find all image tags\n",
    "    for match in img_tag_pattern.finditer(prompt):\n",
    "        image_location = match.group(1)\n",
    "        \n",
    "        try: \n",
    "            img_data = get_image_data(image_location)\n",
    "        except:\n",
    "            # Remove the token\n",
    "            print(f\"Warning! Unable to load image from {image_location}\")\n",
    "            new_prompt = new_prompt.replace(match.group(0), \"\", 1)\n",
    "            continue\n",
    "        \n",
    "        image_locations.append(image_location)\n",
    "        images.append(img_data)\n",
    "        \n",
    "        # Increment the image count and replace the tag in the prompt\n",
    "        new_token = f'<image {image_count}>' if  order_image_tokens else \"<image>\"\n",
    "\n",
    "        new_prompt = new_prompt.replace(match.group(0), new_token, 1)\n",
    "        image_count += 1\n",
    "        \n",
    "    return new_prompt, images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_img_paths(paragraph: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract image paths (URLs or local paths) from a text paragraph.\n",
    "    \n",
    "    Parameters:\n",
    "        paragraph (str): The input text paragraph.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of extracted image paths.\n",
    "    \"\"\"\n",
    "    # Regular expression to match image URLs and file paths\n",
    "    img_path_pattern = re.compile(r'\\b(?:http[s]?://\\S+\\.(?:jpg|jpeg|png|gif|bmp)|\\S+\\.(?:jpg|jpeg|png|gif|bmp))\\b', \n",
    "                                  re.IGNORECASE)\n",
    "    \n",
    "    # Find all matches in the paragraph\n",
    "    img_paths = re.findall(img_path_pattern, paragraph)\n",
    "    return img_paths\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def _to_pil(data):\n",
    "    return Image.open(BytesIO(base64.b64decode(data)))\n",
    "\n",
    "\n",
    "def llava_call(prompt:str, model_name: str=MODEL_NAME, images: list=[], max_new_tokens:int=1000, temperature: float=0.5) -> str:\n",
    "    \"\"\"\n",
    "    Makes a call to the LLaVA service to generate text based on a given prompt and optionally provided images.\n",
    "\n",
    "    Args:\n",
    "        - prompt (str): The input text for the model. Any image paths or placeholders in the text should be replaced with \"<image>\".\n",
    "        - model_name (str, optional): The name of the model to use for the text generation. Defaults to the global constant MODEL_NAME.\n",
    "        - images (list, optional): A list of image paths or URLs. If not provided, they will be extracted from the prompt.\n",
    "            If provided, they will be appended to the prompt with the \"<image>\" placeholder.\n",
    "        - max_new_tokens (int, optional): Maximum number of new tokens to generate. Defaults to 1000.\n",
    "        - temperature (float, optional): temperature for the model. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        - str: Generated text from the model.\n",
    "\n",
    "    Raises:\n",
    "        - AssertionError: If the number of \"<image>\" tokens in the prompt and the number of provided images do not match.\n",
    "        - RunTimeError: If any of the provided images is empty.\n",
    "\n",
    "    Notes:\n",
    "    - The function uses global constants: CONTROLLER_ADDR and SEP.\n",
    "    - Any image paths or URLs in the prompt are automatically replaced with the \"<image>\" token.\n",
    "    - If more images are provided than there are \"<image>\" tokens in the prompt, the extra tokens are appended to the end of the prompt.\n",
    "    \"\"\"\n",
    "    if len(images) == 0:\n",
    "        prompt, images = lmm_formater(prompt, order_image_tokens=False)\n",
    "    else:\n",
    "        # Append the <image> token if missing\n",
    "        assert prompt.count(\"<image>\") <= len(images), \"the number \"\n",
    "        \"of image token in prompt and in the images list should be the same!\"\n",
    "        num_token_missing = len(images) - prompt.count(\"<image>\")\n",
    "        prompt += \" <image> \" * num_token_missing\n",
    "        images = [get_image_data(x) for x in images]\n",
    "    \n",
    "    for im in images:\n",
    "        if len(im) == 0:\n",
    "            raise RunTimeError(\"An image is empty!\")\n",
    "\n",
    "    if LLAVA_MODE == \"local\":\n",
    "        headers = {\"User-Agent\": \"LLaVA Client\"}\n",
    "        pload = {\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"stop\": SEP,\n",
    "            \"images\": images,\n",
    "        }\n",
    "\n",
    "        response = requests.post(CONTROLLER_ADDR + \"/worker_generate_stream\", headers=headers,\n",
    "                json=pload, stream=False)\n",
    "\n",
    "        for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"):\n",
    "            if chunk:\n",
    "                data = json.loads(chunk.decode(\"utf-8\"))\n",
    "                output = data[\"text\"].split(SEP)[-1]\n",
    "    elif LLAVA_MODE == \"remote\":\n",
    "        # The Replicate version of the model only support 1 image for now.\n",
    "        img = 'data:image/jpeg;base64,' + images[0]\n",
    "        response = replicate.run(\n",
    "            \"yorickvp/llava-13b:2facb4a474a0462c15041b78b1ad70952ea46b5ec6ad29583c0b29dbd4249591\",\n",
    "            input={\"image\": img, \"prompt\": prompt.replace(\"<image>\", \" \")}\n",
    "        )\n",
    "        # The yorickvp/llava-13b model can stream output as it's running.\n",
    "        # The predict method returns an iterator, and you can iterate over that output.\n",
    "        output = \"\"\n",
    "        for item in response:\n",
    "            # https://replicate.com/yorickvp/llava-13b/versions/2facb4a474a0462c15041b78b1ad70952ea46b5ec6ad29583c0b29dbd4249591/api#output-schema\n",
    "            output += item\n",
    "        \n",
    "    # Remove the prompt and the space.\n",
    "    output = output.replace(prompt, \"\").strip().rstrip()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "941850d7-dab0-4bf2-a368-41d11cdc4a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background\n",
      "\n",
      "The image shows a toy animal with a flaming mane, which resembles a horse. The animal is wearing glasses, which is a unique and amusing detail. The toy is positioned on a gray surface, which provides a contrast to the bright colors of the flaming mane. The toy is a fascinating and creative representation of a horse with a fiery mane.\n"
     ]
    }
   ],
   "source": [
    "out = llava_call(\"Describe this image: <image>\", \n",
    "                 images=[\"https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png\"])\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a734a9-76ec-44e6-93b8-217ad505a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create a list of OpenAI configuration settings\n",
    "config_list = [\n",
    "  {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"api_key\": \"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\",\n",
    "  }\n",
    "]\n",
    "\n",
    "# Save the configuration list to a file\n",
    "with open(\"OAI_CONFIG_LIST.json\", \"w\") as f:\n",
    "    json.dump(config_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b6d615d-d892-4186-8627-65a1b6be14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AutoGen and setup the LLaVAAgent\n",
    "import autogen\n",
    "from autogen import AssistantAgent, Agent, UserProxyAgent\n",
    "from termcolor import colored\n",
    "import random\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST.json\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "llm_config = {\"config_list\": config_list_gpt4, \"seed\": 42}\n",
    "\n",
    "\n",
    "\n",
    "class LLaVAAgent(AssistantAgent):\n",
    "    def __init__(self, img_str_format=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.register_reply([Agent, None], reply_func=LLaVAAgent._image_reply, position=0)\n",
    "        self._img_str_format = img_str_format\n",
    "\n",
    "    def _image_reply(\n",
    "        self,\n",
    "        messages=None,\n",
    "        sender=None, config=None\n",
    "    ):\n",
    "        # Note: we did not use \"llm_config\" yet.\n",
    "        # TODO 1: make the LLaVA API design compatible with llm_config\n",
    "        # TODO 2: the `_image_reply` function should handle input prompt\n",
    "        #         differently for different `img_str_format`, which will\n",
    "        #         be valueable for different LMMs.\n",
    "        # TODO 3: handle the memory and store image data in the memory.\n",
    "        \n",
    "        if all((messages is None, sender is None)):\n",
    "            error_msg = f\"Either {messages=} or {sender=} must be provided.\"\n",
    "            logger.error(error_msg)\n",
    "            raise AssertionError(error_msg)\n",
    "\n",
    "        if messages is None:\n",
    "            messages = self._oai_messages[sender]\n",
    "\n",
    "        # The formats for LLaVA and GPT are different. So, we manually handle them here.\n",
    "        # TODO: format the images from the history accordingly.\n",
    "        prompt = self.system_message\n",
    "        for msg in messages:\n",
    "            role = \"Human\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            content = msg[\"content\"]\n",
    "            prompt += f\"{SEP}{role}: {content}\"\n",
    "        prompt += SEP\n",
    "        print(colored(prompt, \"blue\"))\n",
    "    \n",
    "        out = \"\"\n",
    "        retry = 10\n",
    "        while len(out) == 0 and retry > 0:\n",
    "            # image names will be inferred automatically from llava_call\n",
    "            out = llava_call(prompt=prompt, temperature=0, max_new_tokens=2000)\n",
    "            retry -= 1\n",
    "            \n",
    "        assert out != \"\", \"Empty response from LLaVA.\"\n",
    "        \n",
    "        \n",
    "        return True, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad0dde6-06aa-43d1-ac5b-236feef7a1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to image-explainer):\n",
      "\n",
      "Here is my image: <img http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0>. Can you give me some suggestions?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34mYou are a garden helper. You should describe the image and provide suggestions for the garden.###Human: Here is my image: <img http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0>. Can you give me some suggestions?###\u001b[0m\n",
      "\u001b[33mimage-explainer\u001b[0m (to User_proxy):\n",
      "\n",
      "Human: Here is my image: <image>. Can you give me some suggestions?Human: Sure, I can give you some suggestions. The image shows a variety of plants, including strawberries, leaves, and other vegetation. Some suggestions for the garden could be:\n",
      "\n",
      "1. Planting more strawberries to increase the yield and variety of the fruit.\n",
      "2. Adding more leaves to the garden to provide shade and improve the overall aesthetic.\n",
      "3. Incorporating a variety of vegetation to create a diverse and healthy ecosystem.\n",
      "4. Ensuring proper drainage and watering to maintain the health of the plants.\n",
      "5. Regularly pruning and maintaining the plants to promote growth and prevent overcrowding.\n",
      "\n",
      "These suggestions can help improve the garden's appearance, productivity, and overall health.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "image_agent = LLaVAAgent(\n",
    "    name=\"image-explainer\",\n",
    "    system_message=\"You are a garden helper. You should describe the image and provide suggestions for the garden.\",\n",
    "    max_consecutive_auto_reply=0\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"groupchat\"\n",
    "    },\n",
    "    human_input_mode=\"NEVER\", # Try between ALWAYS or NEVER\n",
    "    llm_config=llm_config,\n",
    "    max_consecutive_auto_reply=0,\n",
    ")\n",
    "\n",
    "# Ask the question with an image\n",
    "user_proxy.initiate_chat(image_agent, \n",
    "                         message=\"Here is my image: <img http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0>. Can you give me some suggestions?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16163c83-41aa-4bea-bf44-1d297b09e9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
