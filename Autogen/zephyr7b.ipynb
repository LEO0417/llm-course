{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658d9175-5450-4ad7-a383-b0a9a941119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.1.0 which is incompatible.\n",
      "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.1.0 which is incompatible.\n",
      "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install -q git+https://github.com/huggingface/transformers # need to install from github\n",
    "!pip install -q datasets loralib sentencepiece\n",
    "!pip -q install -q  bitsandbytes accelerate xformers einops\n",
    "!pip -q install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ae7cb3-264a-42f6-9b6f-fed255a3aa4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaabddde14964269ba47dd7e1f48181b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/628 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7cbe7aea25415eb6c09bbf1f9f5ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29a94a7f18a45fbb996dc46aa16c8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04a99bdb2274866a75db76167dd98bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eba19c9c5b144428e6c084aa6b700b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdfcb6db4dd44de854e0957eb8b204e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a44409df7b5438992fac51c9a4e520f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc718d268e3c44f785347ed6d72524cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d50c5d040de41a1a2b75cdebad6d492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e7d9f585bc481fbe752c479207e3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b984769646406e909a45a0ae609436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb03985207e48239fdbb880999ced1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1526c7746a244373b12a3f16a44120f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4dc8ed26934bbca53be68699fea328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a653a67594043f196819c07ab46de60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7af658aef3248b1988cdd95e6ca128c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddb6ada4ae448a7a1c19a429191a0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a5df6484184d5eafa994f810303f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16132facbb49471588ca5c7b95a7e01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1471: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful assistant for coding and programming questions.</s>\n",
      "<|user|>\n",
      "Can you explain the concept of recursion in programming?</s>\n",
      "<|assistant|>\n",
      "Sure! Recursion is a programming concept that allows a function to call itself repeatedly until a certain condition is met. The word \"recursion\" itself comes from the Latin word \"recursus,\" which means \"to run back.\"\n",
      "\n",
      "In simpler terms, recursion is a way for a function to solve a problem by breaking it down into smaller and smaller sub-problems until it reaches a base case, which is a point where the function can no longer be called recursively.\n",
      "\n",
      "Here's an example: let's say you want to calculate the factorial of a number using recursion. The factorial of a number is the product of that number and all the positive integers smaller than it.\n",
      "\n",
      "For example, the factorial of 5 is 5 * 4 * 3 * 2 * 1, which equals 120.\n",
      "\n",
      "To calculate the factorial of a number using recursion, we write a function that calls itself until we reach a base case, which is the factorial of 1.\n",
      "\n",
      "Here's an example implementation in Python:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    if n == 1:\n",
      "        return \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "                                             torch_dtype=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "                                          torch_dtype=\"auto\")\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-alpha\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant for coding and programming questions.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Can you explain the concept of recursion in programming?\"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4bf632-3ac4-4f9b-9310-3088ac989dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=90): #preserve_newlines\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca0be44d-7ae6-460d-acd4-e049f848559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_text, system_prompt=\"\",max_length=512):\n",
    "    if system_prompt != \"\":\n",
    "        system_prompt = f\"\"\"<|im_start|> system\\n{system_prompt}<|im_end|>\"\"\"\n",
    "    else:\n",
    "        system_prompt = \"\"\n",
    "    prompt = f\"\"\"<|im_start|> user\\n{input_text}<|im_end|>\"\"\"\n",
    "    final_prompt = system_prompt + prompt\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    model_inputs = encodeds.to(device)\n",
    "    model.to(device)\n",
    "    outputs = model.generate(**inputs,\n",
    "                             max_length=max_length,\n",
    "                             temperature=0.1,\n",
    "                             pad_token_id = 3200,\n",
    "                             do_sample=True)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    # text = text[len(final_prompt):] if text.startswith(final_prompt) else text\n",
    "    text = text.replace(final_prompt, '', 1)\n",
    "    wrapped_text = wrap_text(text)\n",
    "    print(wrapped_text)\n",
    "\n",
    "\n",
    "def generate(input_text, system_prompt=\"\",max_length=512):\n",
    "    if system_prompt != \"\":\n",
    "        system_prompt = system_prompt\n",
    "    else:\n",
    "        system_prompt = \"You are a friendly chatbot who always responds in the style of a pirate\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": input_text},\n",
    "    ]\n",
    "\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages,\n",
    "                                                tokenize=False,\n",
    "                                                add_generation_prompt=True)\n",
    "\n",
    "    outputs = pipe(prompt,\n",
    "                   max_new_tokens=max_length,\n",
    "                   do_sample=True,\n",
    "                   temperature=0.7,\n",
    "                   top_k=50,\n",
    "                   top_p=0.95)\n",
    "    text = outputs[0][\"generated_text\"]\n",
    "    text = text.replace(prompt, '', 1)\n",
    "    wrapped_text = wrap_text(text)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "749a91d4-f1a6-45fe-9dec-41b22cee7d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a possible implementation:\n",
      "\n",
      "```python\n",
      "def is_prime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "def print_prime(n):\n",
      "    for i in range(2, n+1):\n",
      "        if is_prime(i):\n",
      "            print(i)\n",
      "```\n",
      "\n",
      "The `is_prime` function checks if a number is prime by iterating over all numbers between\n",
      "2 and the square root of the number. If the number is divisible by any of these numbers,\n",
      "then it's not prime.\n",
      "\n",
      "The `print_prime` function uses a for loop to iterate over all numbers between 2 and n,\n",
      "and checks if each number is prime using the `is_prime` function. If a prime number is\n",
      "found, it's printed to the console using the `print` function.\n"
     ]
    }
   ],
   "source": [
    "generate('''```python\n",
    "def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', system_prompt=\"You are a genius python coder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "933186bf-fb38-42f2-b96c-a0b8b2ddf08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mathematics is often referred to as a language that speaks in numbers and symbols. It can\n",
      "be seen as a beacon, guiding us through complex concepts and ideas. In the same way, a\n",
      "lighthouse is a beacon that guides ships through dangerous and uncertain waters.\n",
      "\n",
      "Like a lighthouse, mathematics helps us navigate through the unknown. A lighthouse emits a\n",
      "steady beam of light that signals to ships in the distance, providing a clear and reliable\n",
      "path to follow. Similarly, mathematics provides a set of rules and principles that allow\n",
      "us to navigate through complex problems and arrive at logical and reliable solutions.\n",
      "\n",
      "Both mathematics and lighthouses require precision and attention to detail. The light of a\n",
      "lighthouse must be precisely aligned and maintained to ensure that it remains a reliable\n",
      "guide for ships. Similarly, mathematical calculations and formulas must be precise and\n",
      "accurate to ensure that they provide reliable results.\n",
      "\n",
      "In both cases, there is a certain beauty in the precision and simplicity. The light of a\n",
      "lighthouse is a simple, yet elegant, symbol of guidance and safety. Similarly,\n",
      "mathematical formulas can be beautiful and elegant, illustrating the power and simplicity\n",
      "of logical thought.\n",
      "\n",
      "Overall, the analogy between mathematics and a lighthouse highlights the importance of\n",
      "precision, guidance, and safety in both fields. Both mathematics and lighthouses provide\n",
      "us with a clear and reliable path forward, helping us to navigate through complex and\n",
      "uncertain waters with confidence and clarity.\n"
     ]
    }
   ],
   "source": [
    "generate('Write a detailed analogy between mathematics and a lighthouse.',\n",
    "         system_prompt=\"You are Zephyr, a large language model trained by HuggingFace. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5265869-3bb0-4646-9cee-14d1128748a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
