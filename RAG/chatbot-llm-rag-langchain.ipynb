{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7140393,"sourceType":"datasetVersion","datasetId":4121123}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\n! pip install  -U -qq langchain tiktoken pypdf faiss-gpu\n! pip install  -U -qq InstructorEmbedding sentence_transformers","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-12-07T11:48:54.665043Z","iopub.execute_input":"2023-12-07T11:48:54.665920Z","iopub.status.idle":"2023-12-07T11:49:19.599411Z","shell.execute_reply.started":"2023-12-07T11:48:54.665881Z","shell.execute_reply":"2023-12-07T11:49:19.598101Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"CPU times: user 279 ms, sys: 80.7 ms, total: 360 ms\nWall time: 24.9 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Import Stuff","metadata":{}},{"cell_type":"code","source":"import re\nimport os\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nfrom operator import itemgetter\n\nimport langchain\nfrom langchain.schema import format_document\nfrom langchain.schema.messages import get_buffer_string\nfrom langchain.document_loaders import PyPDFLoader, DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain import PromptTemplate, LLMChain\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.vectorstores import FAISS\nfrom langchain.llms import HuggingFacePipeline\nfrom InstructorEmbedding import INSTRUCTOR\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnableLambda, RunnablePassthrough,RunnableParallel\nfrom langchain.schema.messages import HumanMessage, SystemMessage, AIMessage\n\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-07T11:49:19.601393Z","iopub.execute_input":"2023-12-07T11:49:19.601723Z","iopub.status.idle":"2023-12-07T11:49:34.023676Z","shell.execute_reply.started":"2023-12-07T11:49:19.601694Z","shell.execute_reply":"2023-12-07T11:49:34.022585Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import trange\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=42)\n\nimport locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:49:34.024971Z","iopub.execute_input":"2023-12-07T11:49:34.025629Z","iopub.status.idle":"2023-12-07T11:49:34.058367Z","shell.execute_reply.started":"2023-12-07T11:49:34.025598Z","shell.execute_reply":"2023-12-07T11:49:34.057319Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## ChatBot","metadata":{}},{"cell_type":"code","source":"class ChatBot:\n    \n    def __init__(self,\n                 model,\n                 tokenizer,\n                 embeddings,\n                 pdf_path = None,\n                 chat_history = [],\n                 max_len = 1000,\n                 temperature = 0,\n                 top_p = 0.95,\n                 repetition_penalty = 1.15,\n                 split_chunk_size = 800,\n                 split_overlap = 0,\n                 k = 3,\n                 device = \"cuda\",\n                 do_sample = True,\n                 ## vector db\n                 vector_db_save_path = \"faiss_index_hp\",\n                 vector_db_load_path = None,\n                 ## templates\n                 answer_template = None,\n                 condensed_question_template = None,\n                 pipe = None,\n                ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.embeddings = embeddings\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        self.pdf_path = pdf_path\n        self.chat_history = chat_history\n        self.max_len = max_len\n        self.temperature = temperature\n        self.top_p = top_p\n        self.repetition_penalty = repetition_penalty\n        self.split_chunk_size = split_chunk_size\n        self.split_overlap = split_overlap\n        self.k = k\n        self.device = device\n        self.do_sample = do_sample\n        self.vector_db_save_path = vector_db_save_path\n        self.vector_db_load_path = vector_db_load_path\n        \n        \n        # templates\n        if answer_template is None:\n            self.answer_template  = \"\"\"Answer the question based only on the following context:\n                                        {context}\n\n                                        Question: {question}\n                                    \"\"\"\n        self.answer_prompt = ChatPromptTemplate.from_template(self.answer_template)\n        \n        if condensed_question_template is None:\n            self.condensed_question_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n\n                                                Chat History:\n                                                {chat_history}\n                                                Follow Up Input: {question}\n                                                Standalone question:\n                                                \"\"\"\n        \n        self.condensed_question_prompt = PromptTemplate.from_template(self.condensed_question_template)\n        \n        ## llm pipeline \n        if pipe is None:\n            pipe = pipeline(\n                task = \"text-generation\",\n                model = self.model,\n                tokenizer = self.tokenizer,\n                pad_token_id = self.tokenizer.eos_token_id,\n                max_length = self.max_len,\n        #             temperature = self.temperature,\n        #             top_p = self.top_p,\n        #             do_sample=self.do_sample,\n                repetition_penalty = self.repetition_penalty\n            )\n\n        self.llm = HuggingFacePipeline(pipeline = pipe)\n        \n    def load_pdf(self):\n        loader = DirectoryLoader(\n            self.pdf_path,\n            glob=\"./*.pdf\",\n            loader_cls=PyPDFLoader,\n            show_progress=True,\n            use_multithreading=True\n        )\n\n        self.documents = loader.load()\n\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = self.split_chunk_size,\n            chunk_overlap = self.split_overlap\n        )\n\n        self.texts = text_splitter.split_documents(self.documents)        \n        \n    \n    def create_vector_db(self):\n        self.vectordb = FAISS.from_documents(\n            documents = self.texts, \n            embedding = self.embeddings\n        )\n\n        self.vectordb.save_local(self.vector_db_save_path)\n            \n    def load_vector_db(self):\n        self.vectordb = FAISS.load_local(\n            self.vector_db_load_path,\n            self.embeddings\n        )\n        \n    def create_chat_qa_chain(self):\n        \n        if self.vector_db_load_path:\n            self.load_vector_db()\n        else:\n            if self.pdf_path is not None:\n                self.load_pdf()\n                self.create_vector_db()\n                self.vector_db_load_path = self.vector_db_save_path\n                self.load_vector_db()\n        \n        retriever = None\n        if hasattr(self,\"vectordb\"):\n            retriever = self.vectordb.as_retriever(search_kwargs = {\"k\": self.k, \"search_type\" : \"similarity\"})\n        \n        _inputs = RunnableParallel(\n                    standalone_question=RunnablePassthrough.assign(\n                        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n                    )\n                    | self.condensed_question_prompt\n                    | self.llm\n                    | StrOutputParser(),\n                )\n        if retriever is None:\n            _context = {\n                \"context\": itemgetter(\"standalone_question\"),\n                \"question\": lambda x: x[\"standalone_question\"],\n            }\n        else:\n            _context = {\n                \"context\": itemgetter(\"standalone_question\") | retriever,\n                \"question\": lambda x: x[\"standalone_question\"],\n            }\n        \n        self.chain = (\n                _inputs\n                | _context\n                | self.answer_prompt\n                | self.llm\n                |StrOutputParser()\n        )\n    \n    def clean_response(self,response):\n        return response.replace(\"Answer:\",\"\").strip()\n    \n    def start_chat(self):\n        self.create_chat_qa_chain()\n        \n        while True:\n            user_input = input(\"You:\")\n            if user_input.lower().strip() == \"bye\":\n                break\n            model_input = {\n                \"question\": user_input,\n                \"chat_history\": self.chat_history,\n            }\n            response = self.chain.invoke(model_input)\n            print(\"AI:\",self.clean_response(response))\n            #giving context of only last two responses\n            self.chat_history = self.chat_history[-2:]\n            self.chat_history.extend(\n                [\n                    HumanMessage(content=user_input),\n                    AIMessage(content=self.clean_response(response))\n                ]\n            )\n            print()\n            ","metadata":{"execution":{"iopub.status.busy":"2023-12-07T12:01:27.242878Z","iopub.execute_input":"2023-12-07T12:01:27.243243Z","iopub.status.idle":"2023-12-07T12:01:27.267573Z","shell.execute_reply.started":"2023-12-07T12:01:27.243216Z","shell.execute_reply":"2023-12-07T12:01:27.266742Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"seed\":42,\n    \"model_path\":\"mistralai/Mistral-7B-Instruct-v0.1\",\n    \"embeddings_model_path\":\"sentence-transformers/all-MiniLM-L6-v2\",\n    \"pdf_path\":\"/kaggle/input/rag-testing/\",\n    \"vector_db_save_path\":\"faiss_index_hp\",\n    \"vector_db_load_path\":\"faiss_index_hp\",\n    \"device\":\"cuda\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-07T12:01:31.279592Z","iopub.execute_input":"2023-12-07T12:01:31.279973Z","iopub.status.idle":"2023-12-07T12:01:31.284708Z","shell.execute_reply.started":"2023-12-07T12:01:31.279943Z","shell.execute_reply":"2023-12-07T12:01:31.283830Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(config[\"model_path\"],\n                                            device_map='auto',\n                                            torch_dtype=torch.float16,\n                                            low_cpu_mem_usage=True)\ntokenizer = AutoTokenizer.from_pretrained(config[\"model_path\"])\nembeddings = HuggingFaceInstructEmbeddings(model_name = config[\"embeddings_model_path\"],\n                                           model_kwargs = {\"device\":config[\"device\"]})","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:49:34.106733Z","iopub.execute_input":"2023-12-07T11:49:34.107126Z","iopub.status.idle":"2023-12-07T11:51:11.552757Z","shell.execute_reply.started":"2023-12-07T11:49:34.107087Z","shell.execute_reply":"2023-12-07T11:51:11.551744Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e64b30fc536e4ba2bc4552d7bdf92246"}},"metadata":{}},{"name":"stdout","text":"load INSTRUCTOR_Transformer\nmax_seq_length  512\n","output_type":"stream"}]},{"cell_type":"code","source":"chat_bot = ChatBot(model,\n                   tokenizer,\n                   embeddings,\n                   pdf_path=config[\"pdf_path\"],\n                  )","metadata":{"execution":{"iopub.status.busy":"2023-12-07T12:01:33.445468Z","iopub.execute_input":"2023-12-07T12:01:33.445871Z","iopub.status.idle":"2023-12-07T12:01:33.451379Z","shell.execute_reply.started":"2023-12-07T12:01:33.445842Z","shell.execute_reply":"2023-12-07T12:01:33.450422Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"chat_bot.start_chat()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T12:01:33.679469Z","iopub.execute_input":"2023-12-07T12:01:33.679816Z","iopub.status.idle":"2023-12-07T12:04:41.226377Z","shell.execute_reply.started":"2023-12-07T12:01:33.679789Z","shell.execute_reply":"2023-12-07T12:04:41.224619Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:08<00:00,  8.09s/it]\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You: What is the name of the book ?\n"},{"name":"stdout","text":"AI: The Network State\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You: Who wrote this book ?\n"},{"name":"stdout","text":"AI: Balaji Srinivasan\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You: Give me detailed summary of this book\n"},{"name":"stdout","text":"AI: \"The Network State\" is a book written by Balaji Srinivasan that discusses the concept of network states. According to the author, a network state is an organization that operates fully digitally but has real-world capabilities, such as money and a purpose. It can exist as a seed startup or a public company that has been recognized by at least one legacy state. The book argues that network states are feasible and desirable, but not inevitable, and provides examples of how they\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You: Give me names of all the chapters in this book\n"},{"name":"stdout","text":"AI: Chapter 1: Introduction\n                                                    Chapter 2: The Nation State System\n                                                    Chapter 3: The Digital Revolution\n                                                    Chapter 4: The Network State System\n                                                    Chapter 5: From Nation States to Network States\n                                                    Chapter 6: Conclusion\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You: give me detailed summary of chapter 2\n"},{"name":"stdout","text":"AI: Chapter 2 is not mentioned in the provided context.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You: Name of the chapter 2 is The nation state system\n"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"AI: Chapter 2 does not have a name mentioned in the provided context.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You: bye\n"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}