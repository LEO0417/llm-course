- https://github.com/simonw/llm

<img width="734" alt="Screenshot 2023-11-30 at 12 32 12 PM" src="https://github.com/andysingal/modern_nlp_2/assets/20493493/7bb17e51-6c1c-4f72-8b3f-608120b0d362">

https://blog.ouseful.info/2023/11/29/looking-up-ipython-local-llm-ai-magic-promt-history/

<img width="921" alt="Screenshot 2023-11-30 at 12 39 11 PM" src="https://github.com/andysingal/modern_nlp_2/assets/20493493/5bb509be-cf7d-4119-bfe5-1066c9ca4832">


- https://github.com/BlackHC/llm-strategy/tree/main

- Simple and efficient pytorch-native transformer text generation. https://github.com/pytorch-labs/gpt-fast

- GitHub to Hugging Face Hub Dataset Migration Tool https://huggingface.co/spaces/librarian-bots/github-to-huggingface-dataset-migration-tool

- Love to code on your Mac machine, please check : https://github.com/ml-explore/mlx-examples 
<img width="834" alt="Screenshot 2023-12-20 at 7 38 37 AM" src="https://github.com/andysingal/modern_nlp_2/assets/20493493/5af16a5f-b4ef-4fde-9251-0a3018e806fe">

Pro Prompter idea https://makereal.tldraw.com/ 
<img width="1617" alt="Screenshot 2023-12-20 at 7 46 44 AM" src="https://github.com/andysingal/modern_nlp_2/assets/20493493/de3a8950-1d30-4f2c-868b-3a8526df0f6d">

nanoGPT: The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of minGPT that prioritizes teeth over education. Still under active development, but currently the file train.py reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: train.py is a ~300-line boilerplate training loop and model.py a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.
https://github.com/karpathy/nanoGPT
