{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing Necessary Libraries**"
      ],
      "metadata": {
        "id": "olfXYWvjUik8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq plotly sentence-transformers tiktoken langchain openai==0.28 PyPDF2 qdrant_client\n"
      ],
      "metadata": {
        "id": "famy5mAYzipo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bd68cc-0b8a-47d9-f6b3-a49070d6ed8c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries and Setting Up Environment**"
      ],
      "metadata": {
        "id": "HFaUpN_tUngv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Qdrant\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "import qdrant_client as qc\n",
        "import qdrant_client.http.models as qmodels\n",
        "from qdrant_client.http.models import *\n",
        "#from qdrant_client import QdrantClient,models\n",
        "#from qdrant_client.http.models import PointStruct\n",
        "import os\n",
        "import uuid\n",
        "import openai"
      ],
      "metadata": {
        "id": "VEBPtjIv92L2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining File Path for Data Storage**"
      ],
      "metadata": {
        "id": "xqa3M4b8Uzeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/data/\""
      ],
      "metadata": {
        "id": "eKqJgC-vD6Gn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function to Extract Text from PDF Files**"
      ],
      "metadata": {
        "id": "FzxCPPzAU27t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pdf_data(file_path, num_pages = 1):\n",
        "    reader = PdfReader(file_path)\n",
        "    full_doc_text = \"\"\n",
        "    pages = reader.pages\n",
        "    num_pages = len(pages)\n",
        "\n",
        "    try:\n",
        "        for page in range(num_pages):\n",
        "            current_page = reader.pages[page]\n",
        "            text = current_page.extract_text()\n",
        "            full_doc_text += text\n",
        "    except:\n",
        "        print(\"Error reading file\")\n",
        "    finally:\n",
        "        return full_doc_text"
      ],
      "metadata": {
        "id": "cKXA6o2T_DOC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function to Split Text into Manageable Chunks**"
      ],
      "metadata": {
        "id": "khemP5A1U690"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunks(fulltext:str,chunk_length =500) -> list:\n",
        "    text = fulltext\n",
        "\n",
        "    chunks = []\n",
        "    while len(text) > chunk_length:\n",
        "        last_period_index = text[:chunk_length].rfind('.')\n",
        "        if last_period_index == -1:\n",
        "            last_period_index = chunk_length\n",
        "        chunks.append(text[:last_period_index])\n",
        "        text = text[last_period_index+1:]\n",
        "    chunks.append(text)\n",
        "\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "Lp2nyYiaHest"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initializing the Sentence Transformer Model**"
      ],
      "metadata": {
        "id": "FDpqezY3U_JC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "_HMcUcGN-u_x"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVebcorI_Pf1",
        "outputId": "b0e2a9ba-ff00-4788-fea9-6c524b685e36"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
            "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating a Qdrant Connection and Managing Collections**"
      ],
      "metadata": {
        "id": "UwyK5Zf5VF4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tempfile\n"
      ],
      "metadata": {
        "id": "i9MlcncQm3XH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record = 0\n",
        "\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "\n",
        "connection = QdrantClient(\n",
        "    url=\"https://40535b3f-d117-49dd-8bdc-6fb826edc0c6.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"-7PSR7d60KlZ5SlCaHThUM-yST1aGm24lPP0xs9uIkqYn8o85-cRAw\",\n",
        ")"
      ],
      "metadata": {
        "id": "AX8sO7W3KRIP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_name = \"question-answering-v1\""
      ],
      "metadata": {
        "id": "l_VQNtCYRSDf"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collections = connection.get_collections()\n",
        "print(collections)\n",
        "\n",
        "# only create collection if it doesn't exist\n",
        "if collection_name not in [c.name for c in collections.collections]:\n",
        "  connection.create_collection(\n",
        "    collection_name = collection_name,\n",
        "    vectors_config = models.VectorParams(size=768, distance = models.Distance.COSINE)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHklOZL2MfE4",
        "outputId": "d125d7cf-edb7-4820-cb34-6e612e467e9f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collections=[CollectionDescription(name='extractive-question-answering'), CollectionDescription(name='question-answering')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Create collection response:\", connection)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug2QO1adM7Yj",
        "outputId": "a36a3411-93a1-4e2b-8c6d-4d1c813acb7a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create collection response: <qdrant_client.qdrant_client.QdrantClient object at 0x784ae12d6800>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = connection.get_collection(collection_name = collection_name)\n"
      ],
      "metadata": {
        "id": "Y2SaZ98TNBe6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Collection info:\", info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6LccHn6NLWF",
        "outputId": "d8585f0e-9e64-434b-b577-231bbcd28cc5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=0 indexed_vectors_count=0 points_count=0 segments_count=2 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=768, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None) payload_schema={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for get_info in info:\n",
        "  print(get_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4KY2m4lNLqI",
        "outputId": "1417ffe3-d4e3-4983-b6f3-d8d86565a339"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('status', <CollectionStatus.GREEN: 'green'>)\n",
            "('optimizer_status', <OptimizersStatusOneOf.OK: 'ok'>)\n",
            "('vectors_count', 0)\n",
            "('indexed_vectors_count', 0)\n",
            "('points_count', 0)\n",
            "('segments_count', 2)\n",
            "('config', CollectionConfig(params=CollectionParams(vectors=VectorParams(size=768, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None))\n",
            "('payload_schema', {})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_PATH = \"/content/data/\"\n",
        "COLLECTION_NAME = collection_name"
      ],
      "metadata": {
        "id": "3PmeEDRBDfT8"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Upserting Chunks**"
      ],
      "metadata": {
        "id": "mIGYTFEIeiJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILES = os.listdir(FILE_PATH)\n",
        "FILES_FULL_PATH = [FILE_PATH + file for file in FILES]\n",
        "for filename in FILES_FULL_PATH:\n",
        "    print(f'Processing file: {filename}')\n",
        "    full_doc_text = get_pdf_data(filename)\n",
        "    print(f'Full doc text length: {len(full_doc_text)}')\n",
        "    payloads = []\n",
        "    li_id = []\n",
        "    corpus = []\n",
        "    Lines =get_chunks(full_doc_text,500)\n",
        "    for token in Lines:\n",
        "        corpus.append(token)\n",
        "        payloads.append({\"token\":token,\n",
        "                         \"filename\": os.path.basename(filename),\n",
        "                           \"type\":\"pdf\"})\n",
        "        li_id.append(str(uuid.uuid4()))\n",
        "    embeddings_all = model.encode(corpus, convert_to_tensor=True)\n",
        "    print(f'Full embeddings length: {len(embeddings_all)}')\n",
        "\n",
        "    CHUNK_SIZE = 100\n",
        "    for i in range(0, len(embeddings_all), CHUNK_SIZE):\n",
        "        if(i+CHUNK_SIZE > len(embeddings_all) -1):\n",
        "            new_chunk = len(embeddings_all) -1\n",
        "        else:\n",
        "            new_chunk = i+CHUNK_SIZE -1\n",
        "        print(\"Inserting chunk\", i , \"to\", new_chunk)\n",
        "        connection.upsert(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            points=qmodels.Batch(\n",
        "                ids = li_id[i:new_chunk],\n",
        "                vectors=embeddings_all[i:new_chunk].tolist(),\n",
        "                payloads=payloads[i:new_chunk]\n",
        "            ),\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlDL-9OXMp3l",
        "outputId": "2710b009-5083-4f66-8836-01babd67dd2c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: /content/data/dermatologia.pdf\n",
            "Full doc text length: 17780\n",
            "Full embeddings length: 41\n",
            "Inserting chunk 0 to 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K0YcBtz6NVYi"
      },
      "execution_count": 71,
      "outputs": []
    }
  ]
}