{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notion Data Connector and OpenAI Integration (with LlamaIndex)\n",
        "\n",
        "This notebook provides an in-depth exploration of the Notion data connector in LlamaIndex and its integration with OpenAI. We'll cover everything from basic setup to advanced use cases and performance optimization."
      ],
      "metadata": {
        "id": "pyCk9P_xrsJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction and Setup\n",
        "\n",
        "### 1.1 Introduction to Data Connectors in LlamaIndex\n",
        "\n",
        "Data connectors in LlamaIndex are powerful tools that allow you to import data from various sources into your AI applications. They act as bridges between external data repositories (like Notion, Google Drive, or Slack) and LlamaIndex, enabling seamless integration of diverse information into your AI models.\n",
        "\n",
        "Key benefits of data connectors include:\n",
        "- Easy access to data from multiple platforms\n",
        "- Standardized data ingestion process\n",
        "- Ability to keep your AI models up-to-date with the latest information\n",
        "\n",
        "In this notebook, we'll focus on the Notion data connector, demonstrating how to leverage Notion's rich document structure in AI applications."
      ],
      "metadata": {
        "id": "bPzyEzQKrpA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Setup and Installation\n",
        "\n",
        "First, let's install the necessary packages and import the required modules."
      ],
      "metadata": {
        "id": "OUEIfz_mr-r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index llama-index-readers-notion openai fpdf\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "from llama_index.readers.notion import NotionPageReader\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import PromptTemplate\n",
        "from IPython.display import Markdown, display\n",
        "from fpdf import FPDF\n",
        "\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CcdbgKxsGKc",
        "outputId": "fe7eb3c5-43b1-4248-8fd3-1838390d83e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.11.16)\n",
            "Requirement already satisfied: llama-index-readers-notion in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.51.0)\n",
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.4)\n",
            "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.16 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.11.16)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.5)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.48.post3)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.11)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.2)\n",
            "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.0)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.2)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.16->llama-index) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (3.10.8)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (2024.6.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (3.3)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (10.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (0.8.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.16->llama-index) (1.16.0)\n",
            "Requirement already satisfied: llama-cloud>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index) (0.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.3.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse>=0.3.0->llama-index) (0.5.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.16->llama-index) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.16->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.16->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.16->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.16->llama-index) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.16->llama-index) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.16->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.16->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.16->llama-index) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.16->llama-index) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.16->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.16->llama-index) (3.22.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.16->llama-index) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\" # Add your API Key here.\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "wlIv-dVKrkWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"NOTION_INTEGRATION_TOKEN\"] = \"secret_2Javzi8aljUpiy5SF409LFrCg2KOojFrtnF4kndmupy\"\n"
      ],
      "metadata": {
        "id": "87oTbsSvb22u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Notion Data Connector\n",
        "\n",
        "Now, let's explore the Notion data connector in detail."
      ],
      "metadata": {
        "id": "oNYzF-yaevX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notion_reader = NotionPageReader()"
      ],
      "metadata": {
        "id": "aEJ24OnHeta5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Loading Data from Notion Pages\n",
        "\n",
        "We can load data from specific Notion pages using their IDs."
      ],
      "metadata": {
        "id": "DpLviOs6e9o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_ids = [\"115eb6c4652280419508d37521969b68\", \"115eb6c4652280478a32e7ca6b1d82c0\"]\n",
        "page_documents = notion_reader.load_data(page_ids=page_ids)\n",
        "\n",
        "print(f\"Loaded {len(page_documents)} documents from Notion pages\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmnvXuRgeqp3",
        "outputId": "ca074156-4052-44a4-823c-e25ff7319bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2 documents from Notion pages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8zyRj_oymHWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in page_documents:\n",
        "    print(f\"Document ID: {doc.doc_id}\")\n",
        "    print(f\"Document content: {doc.text[:100]}...\")  # Print first 100 characters of the text\n",
        "\n",
        "    if doc.metadata:\n",
        "        print(\"Metadata:\")\n",
        "        for key, value in doc.metadata.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "    if doc.extra_info:\n",
        "        print(\"Extra Info:\")\n",
        "        for key, value in doc.extra_info.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQVolgYgmKZb",
        "outputId": "350cd3b7-19a0-4365-f61d-de9160170eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document ID: 115eb6c4652280419508d37521969b68\n",
            "Document content: Multimodal AI: Vision-Language Models\n",
            "[Last updated: September 28, 2024]\n",
            "Research Overview\n",
            "Developin...\n",
            "Metadata:\n",
            "  page_id: 115eb6c4652280419508d37521969b68\n",
            "Extra Info:\n",
            "  page_id: 115eb6c4652280419508d37521969b68\n",
            "---\n",
            "Document ID: 115eb6c4652280478a32e7ca6b1d82c0\n",
            "Document content: Multimodal AI: Experimental Results and Future Directions\n",
            "Recent Experimental Findings\n",
            "Just finished...\n",
            "Metadata:\n",
            "  page_id: 115eb6c4652280478a32e7ca6b1d82c0\n",
            "Extra Info:\n",
            "  page_id: 115eb6c4652280478a32e7ca6b1d82c0\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Processing and Indexing\n",
        "\n",
        "Now that we have our Notion data, let's process and index it for efficient querying."
      ],
      "metadata": {
        "id": "Ucuh26mGmvN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(page_documents)"
      ],
      "metadata": {
        "id": "IrNCCGVOmx1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.storage_context.persist(\"notion_index\")"
      ],
      "metadata": {
        "id": "CDZ2eVUSmz_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "storage_context = StorageContext.from_defaults(persist_dir=\"notion_index\")\n",
        "loaded_index = load_index_from_storage(storage_context)"
      ],
      "metadata": {
        "id": "TGXHqZwtm2Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. OpenAI Integration\n",
        "\n",
        "We'll now set up the OpenAI integration for advanced querying and summarization."
      ],
      "metadata": {
        "id": "9TGHEjoLm4Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.7, model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "cq-542Exm2pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = loaded_index.as_query_engine(\n",
        "    llm=llm,\n",
        "    response_mode=\"tree_summarize\"\n",
        ")"
      ],
      "metadata": {
        "id": "5FgmfPwxm54g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_prompt = PromptTemplate(\n",
        "    \"You are an AI assistant answering questions about Notion documents. \"\n",
        "    \"Context information is below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given this information, please answer the question: {query_str}\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "4zp8kavrm7TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = loaded_index.as_query_engine(\n",
        "    llm=llm,\n",
        "    text_qa_template=custom_prompt,\n",
        "    response_mode=\"tree_summarize\"\n",
        ")"
      ],
      "metadata": {
        "id": "imDUXxzQm9j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Querying the Notion Data\n",
        "\n",
        "Let's start with some basic queries to our Notion data."
      ],
      "metadata": {
        "id": "gylnsrtcnErr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = query_engine.query(\"What are the main research findings or conclusions from these Notion documents?\")\n",
        "display(Markdown(f\"**Key Findings:** {response1}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "Sydf2wgwnDa_",
        "outputId": "16b11e8d-6adf-4be9-b62b-597a464829b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Key Findings:** The research findings and conclusions from the Notion documents include the development of advanced vision-language models for multimodal AI applications with a focus on improving zero-shot image classification, enhancing cross-modal attention mechanisms, and optimizing fine-tuning for domain-specific tasks. The experiments have shown promising results in utilizing CLIP-inspired contrastive learning, improving alignment between visual and textual features through cross-modal attention, and achieving a performance boost through fine-tuning on domain-specific datasets. Ablation studies have revealed the contributions of different attention mechanisms and pretraining objectives to model performance, while challenges such as computational resources, data quality, and evaluation metrics have been identified. Future research directions involve exploring few-shot learning, multimodal reasoning, temporal understanding, and ethical considerations in AI systems. The goal is to address gaps in performance, scalability, and real-world applicability of vision-language models for multimodal AI tasks."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = query_engine.query(\"What research methods or approaches are mentioned in these documents?\")\n",
        "display(Markdown(f\"**Research Methods:** {response2}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "Y5PqFQHInAKb",
        "outputId": "fee45b28-cd7a-425c-bdf5-480639f575ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Research Methods:** The research methods or approaches mentioned in these documents include developing advanced vision-language models for multimodal AI applications, improving zero-shot image classification, enhancing cross-modal attention mechanisms, optimizing fine-tuning for domain-specific tasks, contrastive learning approach for training, experimenting with different attention mechanisms (self, cross, multi-head), using a Transformer-based architecture with cross-modal attention layers, implementing curriculum learning for more efficient training, exploring few-shot learning capabilities in new domains, investigating potential for image generation tasks, developing interpretability tools for cross-modal attention patterns, applying the model to video understanding, exploring attention mechanisms for long-range temporal dependencies, and exploring neuro-symbolic approaches to enhance logical reasoning."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Advanced Querying and Analysis\n",
        "\n",
        "Now, let's perform some more advanced queries and analysis on our Notion data."
      ],
      "metadata": {
        "id": "voFmWUR3nMN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response3 = query_engine.query(\"Are there any significant dates, deadlines, or timelines mentioned in the research notes?\")\n",
        "display(Markdown(f\"**Important Dates:** {response3}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "Tjx1fuK-nJHL",
        "outputId": "89836ec5-de79-41f9-a1a6-251b01cb791d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Important Dates:** Important dates mentioned in the research notes include:\n- March 15, 2024: Project kickoff\n- April 30, 2024: Initial data collection complete\n- May 1 - July 31, 2024: First round of model training\n- August 1 - September 15, 2024: Comprehensive evaluation and analysis\n- October 1, 2024: Target date for paper submission (ICLR deadline)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response4 = query_engine.query(\"What open questions or areas for further research are identified in these documents?\")\n",
        "display(Markdown(f\"**Future Research Directions:** {response4}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "DXQgUcsEnOnD",
        "outputId": "6c2c4452-1d05-4f79-fbd3-ca43945c3fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Future Research Directions:** Further research areas identified in the documents include exploring few-shot learning in new domains, investigating image generation tasks using the vision-language model, developing interpretability tools for cross-modal attention patterns, potential application of the model to video understanding, addressing challenges related to computational resources and data quality, improving evaluation metrics for multimodal understanding, enhancing multimodal reasoning capabilities, extending the model for temporal understanding in videos, incorporating ethical considerations for bias mitigation in multimodal systems, and exploring new research directions such as pretraining on scientific papers, multi-task learning, and cross-lingual multimodal models."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response5 = query_engine.query(\"What are the most frequently cited sources or references in these research documents?\")\n",
        "display(Markdown(f\"**Key References:** {response5}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "id": "d8U9XI0_nQm3",
        "outputId": "a5b559ee-b824-43de-9d3f-f98ca9f376b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Key References:** Radford, A., et al. (2021), Chen, Y., et al. (2020), and Gebru, T., et al. (2020) are among the most frequently cited sources or references in these research documents."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Comprehensive Summarization\n",
        "\n",
        "Let's generate a comprehensive summary of all the Notion documents."
      ],
      "metadata": {
        "id": "CbvJ9m-inZcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a custom prompt template\n",
        "custom_prompt = PromptTemplate(\n",
        "    \"\"\"Based on the following information from research documents, create a comprehensive and insightful summary:\n",
        "\n",
        "Key Findings: {findings}\n",
        "Research Methods: {methods}\n",
        "Important Dates: {dates}\n",
        "Future Research Directions: {future_research}\n",
        "Key References: {references}\n",
        "\n",
        "Please synthesize this information into a coherent summary that highlights the most important aspects of the research, identifies any patterns or connections between different elements, and provides a holistic overview of the work. Structure the summary with appropriate headings and ensure it flows logically.\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Combine all responses into a single text\n",
        "combined_text = f\"\"\"\n",
        "Key Findings: {response1}\n",
        "Research Methods: {response2}\n",
        "Important Dates: {response3}\n",
        "Future Research Directions: {response4}\n",
        "Key References: {response5}\n",
        "\"\"\"\n",
        "\n",
        "# Generate the summary\n",
        "response = query_engine.query(\"Summarize the research project\")\n",
        "\n",
        "# Display the generated summary\n",
        "print(\"AI-Generated Research Summary:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_L0v2slnVls",
        "outputId": "bfa80e91-5730-4bb7-b494-2d413416212f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI-Generated Research Summary:\n",
            "The research project focuses on developing advanced vision-language models for multimodal AI applications. The key objectives include improving zero-shot image classification, enhancing cross-modal attention mechanisms, and optimizing fine-tuning for domain-specific tasks. The project utilizes a curated dataset of image-text pairs, a Transformer-based architecture with cross-modal attention layers, and a contrastive learning approach for training. Initial findings show promising results with CLIP-inspired contrastive learning and the significance of cross-modal attention in aligning visual and textual features. The project aims to explore few-shot learning capabilities, investigate image generation tasks, and develop interpretability tools for cross-modal attention patterns. Future directions also include exploring multimodal reasoning, handling temporal dependencies in video inputs, and addressing ethical considerations in multimodal systems. The project has conducted experiments, ablation studies, and aims to present state-of-the-art results at the upcoming ICLR conference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also add this back to our Notion page."
      ],
      "metadata": {
        "id": "xeqkkVBsO_-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Use the existing Notion integration token\n",
        "NOTION_API_KEY = os.environ[\"NOTION_INTEGRATION_TOKEN\"]\n",
        "\n",
        "# Function to update a Notion page\n",
        "def update_notion_page(page_id, summary_text):\n",
        "    url = f\"https://api.notion.com/v1/blocks/{page_id}/children\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {NOTION_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Notion-Version\": \"2022-06-28\"  # Use the latest API version\n",
        "    }\n",
        "\n",
        "    # Prepare the data for the update\n",
        "    data = {\n",
        "        \"children\": [\n",
        "            {\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"paragraph\",\n",
        "                \"paragraph\": {\n",
        "                    \"rich_text\": [{\"type\": \"text\", \"text\": {\"content\": summary_text}}]\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.patch(url, headers=headers, data=json.dumps(data))\n",
        "    return response.json()\n",
        "\n",
        "# Assuming 'response' contains your generated summary\n",
        "summary_text = str(response)  # Convert the response to a string if it's not already\n",
        "\n",
        "# Update the first Notion page with the summary\n",
        "first_page_id = page_ids[0]  # Get the ID of the first page\n",
        "result = update_notion_page(first_page_id, summary_text)\n",
        "\n",
        "if 'results' in result:\n",
        "    print(f\"Successfully updated Notion page: {first_page_id}\")\n",
        "else:\n",
        "    print(f\"Failed to update Notion page. Error: {result.get('message', 'Unknown error')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFftgIv6OOGC",
        "outputId": "9f254e8f-cdac-49ed-b7b7-2f3f55557160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully updated Notion page: 115eb6c4652280419508d37521969b68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Exporting Results as PDF\n",
        "\n",
        "Now, we'll export our summary as a PDF"
      ],
      "metadata": {
        "id": "uVqMiXG5nf4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_to_pdf(content, filename=\"notion_summary.pdf\"):\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.multi_cell(0, 10, content)\n",
        "    pdf.output(filename)\n",
        "    print(f\"Content exported as {filename}\")"
      ],
      "metadata": {
        "id": "7JgpOe3VnSIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_to_pdf(str(response), \"notion_comprehensive_summary.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEfKYmRBnia1",
        "outputId": "a002580e-fbe1-4eb2-e0b8-7be04a81e972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content exported as notion_comprehensive_summary.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "additional_insights = query_engine.query(\"Provide additional insights, trends, or patterns observed across all documents that weren't included in the main summary.\")\n",
        "export_to_pdf(str(additional_insights), \"notion_additional_insights.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bUrAKZpnkXl",
        "outputId": "a8776438-1c59-43b6-81e3-63f67af9216a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content exported as notion_additional_insights.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Conclusion and Next Steps\n",
        "\n",
        "In this project, we've accomplished:\n",
        "1. Loading and processing research data from Notion\n",
        "2. Generating comprehensive summaries using LlamaIndex and OpenAI\n",
        "3. Exporting results as a PDF summary\n",
        "\n",
        "Next steps:\n",
        "1. Integrate with Overleaf for LaTeX-based report generation\n",
        "2. Implement automated literature review features\n",
        "3. Develop interactive visualizations of research findings\n",
        "4. Explore multi-language support for international research\n",
        "5. Optimize performance for larger datasets\n"
      ],
      "metadata": {
        "id": "EVXKY56hsrzm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1SizTqsqsrnD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}