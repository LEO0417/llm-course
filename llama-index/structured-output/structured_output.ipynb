{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ybtH4tvY-7Oi"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr --no-display\n",
        "!pip install langgraph langchain_community langchain_openai llama_index llama-index-llms-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = ''\n",
        "groq_api_key = os.environ[\"GROQ_API_KEY\"] = ''"
      ],
      "metadata": {
        "id": "fSMSFfDf_MEr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "llama3 = Groq(model=\"llama3-8b-8192\", api_key=groq_api_key, temperature=0.0)\n",
        "\n",
        "llama3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHh96ZxsBTi6",
        "outputId": "e864ecb8-7de9-42c4-e038-a3234ca99207"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Groq(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7d4d40336c80>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7d4d112575b0>, completion_to_prompt=<function default_completion_to_prompt at 0x7d4d110ebf40>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='llama3-8b-8192', temperature=0.0, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='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', api_base='https://api.groq.com/openai/v1', api_version='', context_window=3900, is_chat_model=True, is_function_calling_model=True, tokenizer=None)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Class Representing Individual Person.\"\"\"\n",
        "\n",
        "    name: str = Field(description=\"Name of the person.\")\n",
        "    age: int = Field(description=\"Age of Person.\")\n",
        "    height: Optional[str] = Field(description=\"Height of Person\")\n",
        "\n",
        "class People(BaseModel):\n",
        "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
        "\n",
        "    people: List[Person]"
      ],
      "metadata": {
        "id": "43Vp4NnMBEUv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Structured Output"
      ],
      "metadata": {
        "id": "HP9IICKJBJ9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.program import LLMTextCompletionProgram, FunctionCallingProgram\n",
        "from llama_index.core.output_parsers import PydanticOutputParser\n",
        "\n",
        "prompt_template_str = \"\"\"\\\n",
        "You are an expert data parser. Parse data from user query.\n",
        "\n",
        "If you don't know any field then set it to None.\n",
        "\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "pydantic_data_parser = LLMTextCompletionProgram.from_defaults(\n",
        "    output_parser=PydanticOutputParser(Person),\n",
        "    #output_cls=Person,\n",
        "    prompt_template_str=prompt_template_str,\n",
        "    llm=llama3\n",
        ")"
      ],
      "metadata": {
        "id": "j1XBLq_FBKr6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = pydantic_data_parser(query=\"Anna is 20 years old and five foot five inch.\")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiqkU7UCCn9q",
        "outputId": "84bb55e1-e868-45a9-f8c5-cd514e7061cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Person(name='Anna', age=20, height='five foot five inch')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SewrJa1sCoV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}