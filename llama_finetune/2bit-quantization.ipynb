{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-_k4j9wm5GD"
      },
      "source": [
        "## AQLM inference example\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/colab_example.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6egoxPVyckBF"
      },
      "source": [
        "**Install the requirements**\n",
        "- `aqlm` is the only extra dependency to run AQLM models.\n",
        "- Install the latest `accelerate` to pull the latest bugfixes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A584OAwRWGks"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install aqlm[gpu]==1.0.0\n",
        "!pip install git+https://github.com/huggingface/accelerate.git@main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTfcs4lrc1x4"
      },
      "source": [
        "**Load the model as usual**\n",
        "\n",
        "Just don't forget to add:\n",
        " - `trust_remote_code=True` to pull the inference code.\n",
        " - `torch_dtype=\"auto\"` to load the model in it's native dtype.\n",
        " - `device_map=\"cuda\"` to load the model on GPU straight away, saving RAM.\n",
        "\n",
        "The tokenizer is just a normal `Mixtral` tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lecaItWkVpIC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"BlackSamorez/Mixtral-8x7b-AQLM-2Bit-1x16-hf\",\n",
        "    trust_remote_code=True, torch_dtype=\"auto\", device_map=\"cuda\"\n",
        ").cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39QpRiPbcBYa"
      },
      "source": [
        "Do a few forward passes to load CUDA and automatically compile the kernels. It's done separately here for it not to affect the generation speed benchmark below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii-mWRdQZCOF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "output = quantized_model.generate(tokenizer(\"\", return_tensors=\"pt\")[\"input_ids\"].cuda(), max_new_tokens=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOQfeb_ScIyb"
      },
      "source": [
        "**Measure generation speed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyl4uCxTdmKi",
        "outputId": "17f487b0-54c7-4145-9f57-b82f16c5c267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 7.21 s, sys: 104 ms, total: 7.31 s\n",
            "Wall time: 7.38 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "output = quantized_model.generate(tokenizer(\"I'm AQLM, \", return_tensors=\"pt\")[\"input_ids\"].cuda(), min_new_tokens=128, max_new_tokens=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G5E4tmVdpLF"
      },
      "source": [
        "Note that `transformers` generation is not the fastest implementation and it's heavily influenced by CPU capabilities of _Google Colab_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvShqlguccep"
      },
      "source": [
        "**Check that the output is what one would expect from Mixtral**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsOmDVBvXobJ",
        "outputId": "b796a7a7-fe24-4d73-98e3-4a1d67d7fb82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> I'm AQLM, 20 years old, and I'm from the Netherlands. I'm a student and I'm currently studying at the University of Amsterdam. I'm a very active person and I love to meet new people. I'm a very open person and I'm always looking for new things to do. I'm a very active person and I love to meet new people. I'm a very open person and I'm always looking for new things to do. I'm a very active person and I love to meet new people. I'm a very open person and I'm always looking for new\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6SuiDKYmpCq"
      },
      "source": [
        "**Check peak memory usage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDOtpnJGizsx",
        "outputId": "0fe816b4-98fa-4e65-97fe-5795344a19c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak memory usage: 2.62 Gb\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"Peak memory usage: {torch.cuda.max_memory_allocated()*1e-9:.2f} Gb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNAxWzMWmr3T"
      },
      "source": [
        "Indeed, it's ~2 bits per model weight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNxm4nx3m79V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}