Fine Tune Examples

[Fine Tune LLM with Axolotl](https://github.com/andysingal/llm-course/blob/main/llama_finetune/Fine_tune_LLMs_with_Axolotl.ipynb)

[Template-free axolotl](https://hamel.dev/notes/llm/finetuning/09_template_free.html)

[Finetune_7B_dpo.ipynb](https://github.com/andysingal/llm-course/blob/main/llama_finetune/Finetune_7B_dpo.ipynb)

[Finetune-tech-keywords](https://github.com/andysingal/llm-course/blob/main/llama_finetune/finetune_tech_keywords.ipynb)

[Fine_tune_SegFormer](https://github.com/andysingal/llm-course/blob/main/llama_finetune/Fine_tune_SegFormer_on_custom_dataset.ipynb)

[Supervise fine-tune casual language model](https://www.kaggle.com/code/aisuko/supervise-fine-tune-casual-language-model)

[Fine-Tune LLMs in 2024](https://www.determined.ai/blog/llm-finetuning)

[FineTune-RoBERTa-LORA](https://new.qq.com/rain/a/20240213A02E8M00)

[Phi_2_+_QLoRA+dialogsum](https://github.com/andysingal/llm-course/blob/main/llama_finetune/Phi_2_%2B_QLoRA%2Bdialogsum.ipynb)

[RoBERTa Sequence Classification Fine-Tuning with LORA]

[Fine-Tuning-Custom-Dataset](https://github.com/andysingal/llm-course/blob/main/llama_finetune/fine-tuning-custom-dataset.ipynb)

[AutoLOra-Mergimg](https://github.com/andysingal/llm-course/blob/main/llama_finetune/%F0%9F%A7%9C_AutoLoRAMerging_(Ties%2C_Dare%2C_MagnitudePrune).ipynb) 

[2-bit-quantization](https://github.com/andysingal/llm-course/blob/main/llama_finetune/2_bit_Quantization.ipynb)

[Fine-Tuning Gemma Models in Hugging Face](https://github.com/andysingal/llm-course/blob/main/llama_finetune/examples_notebook_sft_peft.ipynb)

[Fine-Tune Gemma with ChatML](https://github.com/andysingal/llm-course/blob/main/llama_finetune/gemma-lora-example.ipynb)


Resources:
- [GGUF/GGML/Llama.cpp](https://github.com/andysingal/llm-course/blob/main/llama_finetune/file_types.md)
- [Determined-AI-LLM](https://www.determined.ai/blog/llm-finetuning)
- [ViT-fine-tuning](https://github.com/olonok69/LLM_Notebooks/blob/main/image/Image_classification_NSWF_full_training.ipynb)
- [Using Wandb](https://wandb.ai/site/solutions/llm-fine-tuning)
- [Finetune-flashatten](https://medium.com/@yernenip/optimizing-phi-2-a-deep-dive-into-fine-tuning-small-language-models-9d545ac90a99)
- [Exploring mergekit for Model Merge, AutoEval for Model Evaluation, and DPO for Model Fine-tuning](https://medium.com/towards-data-science/exploring-mergekit-for-model-merge-and-autoeval-for-model-evaluation-c681766fd1f3)


<img width="900" alt="Screenshot 2024-02-05 at 4 11 08â€¯PM" src="https://github.com/andysingal/llm-course/assets/20493493/81eef3c8-d3a5-4a3b-a0dd-174dd7c65de1">

## Memory optimization
### Vllm
- https://vllm.readthedocs.io/_/downloads/en/latest/pdf/ 
- https://sumanthrh.com/post/distributed-and-efficient-finetuning/ 
- https://www.philschmid.de/deepspeed-lora-flash-attention 
- https://mer.vin/2024/02/flash-attention-2/
- https://quickaitutorial.com/five-technique-vllm-torch-flash_attention-super-local-llm/ 
