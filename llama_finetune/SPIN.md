An important method for the training of large language models is the alignment method, which includes supervised fine-tuning (SFT) using human samples and human feedback reinforcement learning that relies on human preferences. (RLHF). These methods play a crucial role in llm, but alignment methods have a large demand for manually annotated data. This challenge has made fine-tuning a vibrant area of ​​research, with researchers actively working to develop methods that can effectively exploit human data.

### SPIN
SPIN is like a two-player game. In this game:

Master Model (New LLM) - The role of this agent is to learn how to differentiate between responses generated by a language model (LLM) and responses created by humans. In each iteration, the main model is the LLM being actively trained. The goal is to improve its ability to identify and differentiate responses.

Adversary Model (old LLM) - The adversary model is tasked with generating results that are indistinguishable from responses produced by humans. The adversary model is the LLM from the previous iteration (round). It uses a self-game mechanism to produce results based on past knowledge. The goal of the opponent model is to create realistic reactions so that the new LLM cannot tell whether it is machine-generated or not.

Is this process very similar to GAN, but it’s still different?

The dynamics of SPIN involves the use of a supervised fine-tuning (SFT) dataset, which consists of input (x) and output (y) pairs. These examples are annotated by humans and serve as the basis for training the main model to recognize human-like responses. Some public SFT datasets include Dolly15K, Baize, Ultrachat, etc.

Main model training

To train the main model to distinguish between language models (LLMs) and human responses, SPIN uses an objective function. This function measures the expected value gap between the real data and the response produced by the adversary model. The goal of the main model is to maximize this expected value gap. This involves assigning high values ​​to cues paired with responses from real data, and assigning low values ​​to response pairs generated by the adversary model. This objective function is formulated as a minimization problem.

The master model's job is to minimize a loss function that measures the difference between the pairwise assignment values ​​from the real data and the pairwise assignment values ​​from the opponent model's responses. Throughout the training process, the master model adjusts its parameters to minimize this loss function. This iterative process continues until the master model is proficient in effectively distinguishing LLM responses from human responses.

Updates to opponent models

Updating the adversary model involves improving the ability of the master model, which during training has learned to distinguish between real data and language model responses. As the master model improves and its understanding of specific function classes is improved, we also need to update parameters such as the adversary model. When the master player is faced with the same prompts, it uses its learned discrimination to evaluate their value.

The goal of the opponent model player is to enhance the language model so that its responses are indistinguishable from the master player's real data. This requires setting up a process to adjust the parameters of the language model. The goal is to maximize the master model's evaluation of the language model's response while maintaining stability. This involves a balancing act, ensuring that improvements don't stray too far from the original language model.

It sounds a bit confusing, so let’s briefly summarize it:

There is only one model during training, but the model is divided into the previous round model (old LLM/opponent model) and the main model (being trained), and the output of the model being trained is compared with the output of the previous round model. to optimize the training of the current model. But here we are required to have a trained model as the opponent model, so the SPIN algorithm is only suitable for fine-tuning the training results.

<img width="718" alt="Screenshot 2024-02-15 at 9 54 58 AM" src="https://github.com/andysingal/llm-course/assets/20493493/4532fb14-ebd2-4bda-bcb1-05c17d8532b0">


<img width="711" alt="Screenshot 2024-02-15 at 10 24 13 AM" src="https://github.com/andysingal/llm-course/assets/20493493/eba1deb5-6447-4cc6-9b70-ee4b8a9737fe">

Resources:
- https://segmentfault.com/a/1190000044565620  
