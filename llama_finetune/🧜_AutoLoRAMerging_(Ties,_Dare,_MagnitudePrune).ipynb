{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title # üßú‚Äç‚ôÄÔ∏è AutoLoRA-Merging\n",
        "\n",
        "# @markdown Merge multipe LoRA adapters together with `Ties`, `DareLinear`, `MagnitudePrune`, `Cat`,\n",
        "\n",
        "# @markdown `Linear`, `SVD` merging techniques.\n",
        "\n",
        "# @markdown üîÆ Created by [@zainulabideen](https://huggingface.co/abideen).\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### ü§ó Hugging Face Hub\n",
        "\n",
        "MODEL_ID = \"abideen/MulitLoRA-Mistral-Merging\" # @param {type:\"string\"}\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
        "# username = \"abideen\" # @param {type:\"string\"}\n",
        "token = \"\" # @param {type:\"string\"}\n",
        "!pip install -q huggingface_hub\n",
        "from huggingface_hub import create_repo, HfApi\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### ‚ö° Merging parameters\n",
        "\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install git+https://github.com/huggingface/peft\n",
        "!pip install datasets accelerate bitsandbytes\n",
        "\n",
        "\n",
        "from peft import PeftConfig, PeftModel\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import random\n",
        "\n",
        "PEFT_MODEL_ID = \"Yhyu13/dolphin-2.6-mistral-7b-dpo-laser-function-calling-lora\" # @param {type:\"string\"}\n",
        "FIRST_ADAPTER_NAME = \"functioncalling\" # @param {type:\"string\"}\n",
        "SECOND_ADAPTER = \"predibase/wikisql\" # @param {type:\"string\"}\n",
        "THIRD_ADAPTER = \"predibase/legal\" # @param {type:\"string\"}\n",
        "device_map = {\"\": 0}\n",
        "config = PeftConfig.from_pretrained(PEFT_MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, device_map=device_map)\n",
        "tokenizer = AutoTokenizer.from_pretrained(PEFT_MODEL_ID)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = PeftModel.from_pretrained(model, PEFT_MODEL_ID, adapter_name=FIRST_ADAPTER_NAME)\n",
        "_ = model.load_adapter(SECOND_ADAPTER, adapter_name=SECOND_ADAPTER)\n",
        "_ = model.load_adapter(THIRD_ADAPTER, adapter_name=THIRD_ADAPTER)\n",
        "\n",
        "adapters = [FIRST_ADAPTER_NAME, SECOND_ADAPTER, THIRD_ADAPTER]\n",
        "weights = [2.0, 0.3, 0.7]\n",
        "adapter_name = \"merge\"\n",
        "MERGING_TYPE = \"magnitude_prune\" # @param [\"ties\", \"dare_linear\", \"magnitude_prune\", \"cat\", \"linear\", \"svd\"]\n",
        "DENSITY = 0.2 # @param {type:\"number\"}\n",
        "if adapter_name in model.peft_config:\n",
        "    model.delete_adapter(adapter_name)\n",
        "model.add_weighted_adapter(adapters, weights, adapter_name, combination_type=combination_type, density=density)\n",
        "model.eval()\n",
        "model.set_adapter(\"merge\")\n",
        "prompt = \"Table: Sports; Columns: ['Team', 'Head Coach', 'President', 'Home Ground', 'Location'] Natural Query: Who is the Head Coach of the team whose President is Mario Volarevic? SQL Query:\" # @param {type:\"string\"}\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")  # , add_special_tokens=False)\n",
        "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.2,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "print(tokenizer.decode(outputs[0]))\n",
        "output=\"./out\"\n",
        "model.save_pretrained(output)\n",
        "tokenizer.save_pretrained(output)\n",
        "merge_file=\"./out/merge\"\n",
        "api = HfApi()\n",
        "\n",
        "# Create empty repo\n",
        "create_repo(\n",
        "    repo_id = MODEL_ID,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "# Upload gguf files\n",
        "api.upload_folder(\n",
        "    folder_path=merge_file,\n",
        "    repo_id=MODEL_ID,\n",
        "    token=hf_token\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A2XaB0GaQYv_"
      },
      "id": "A2XaB0GaQYv_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}