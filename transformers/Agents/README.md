
Transformers Agents is an experimental API which is subject to change at any time. Results returned by the agents can vary as the APIs or underlying models are prone to change. 

It provides a natural language API on top of transformers: we define a set of curated tools and design an agent to interpret natural language and to use these tools. It is extensible by design; we curated some relevant tools, but we’ll show you how the system can be extended easily to use any tool developed by the community.

## Agents
The “agent” here is a large language model, and we’re prompting it so that it has access to a specific set of tools.

LLMs are pretty good at generating small samples of code, so this API takes advantage of that by prompting the LLM gives a small sample of code performing a task with a set of tools. This prompt is then completed by the task you give your agent and the description of the tools you give it. This way it gets access to the doc of the tools you are using, especially their expected inputs and outputs, and can generate the relevant code.

## Tools
Tools are very simple: they’re a single function, with a name, and a description. We then use these tools’ descriptions to prompt the agent. Through the prompt, we show the agent how it would leverage tools to perform what was requested in the query.

This is using brand-new tools and not pipelines, because the agent writes better code with very atomic tools. Pipelines are more refactored and often combine several tasks in one. Tools are meant to be focused on one very simple task only.

## Code-execution?!
This code is then executed with our small Python interpreter on the set of inputs passed along with your tools. We hear you screaming “Arbitrary code execution!” in the back, but let us explain why that is not the case.

The only functions that can be called are the tools you provided and the print function, so you’re already limited in what can be executed. You should be safe if it’s limited to Hugging Face tools.

Then, we don’t allow any attribute lookup or imports (which shouldn’t be needed anyway for passing along inputs/outputs to a small set of functions) so all the most obvious attacks (and you’d need to prompt the LLM to output them anyway) shouldn’t be an issue. If you want to be on the super safe side, you can execute the run() method with the additional argument return_code=True, in which case the agent will just return the code to execute and you can decide whether to do it or not.

The execution will stop at any line trying to perform an illegal operation or if there is a regular Python error with the code generated by the agent.




<img width="885" alt="Screenshot 2024-04-30 at 3 51 54 PM" src="https://github.com/andysingal/llm-course/assets/20493493/c3d37ca3-8565-40b9-accb-28c80d25cb72">

<img width="960" alt="Screenshot 2024-04-30 at 3 53 25 PM" src="https://github.com/andysingal/llm-course/assets/20493493/e5eaffd6-4fe7-471d-8140-88acbbb1cf47">

### Quickstart

```py
pip install transformers[agents]
pip install openai

from transformers import OpenAiAgent

from transformers import HfAgent

from huggingface_hub import login

login("<YOUR_TOKEN>")

1. agent = OpenAiAgent(model="text-davinci-003", api_key="<your_api_key>")

## To use BigCode or OpenAssistant, start by logging in to have access to the Inference API:
2. # Starcoder
agent = HfAgent("https://api-inference.huggingface.co/models/bigcode/starcoder")

agent.run("Draw me a picture of rivers and lakes.")
```

