In deep learning, we often use the terms embedding vectors, representations, and latent space. What do these concepts have in common, and how do they differ?

While these three terms are often used interchangeably, we can make subtle distinctions between them:

- Embedding vectors are representations of input data where similar items are close to each other. Alternatively, Embedding vectors, or embeddings for short, encode relatively high-dimensional data into relatively low-dimensional vectors.
Embeddings can have higher or lower numbers of dimensions than the original input. For instance, using embeddings methods for extreme expression, we can encode data into two-dimensional dense and continuous representations for visualization purposes and clustering analysis
<img width="692" alt="Screenshot 2024-04-21 at 8 57 35â€¯PM" src="https://github.com/andysingal/llm-course/assets/20493493/d12de310-421f-4c38-bb14-32295e1db987">

- Latent space is typically used synonymously with embedding space, the space into which embedding vectors are mapped.

- Representations are encoded versions of the original input.

-- To be continued 
