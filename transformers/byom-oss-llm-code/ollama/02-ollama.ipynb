{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a5a74d",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this notebook, we will test out pulling out Microsoft's [Phi3:14b](https://ollama.com/library/phi3) and serving with [Ollama](https://ollama.com/) in SAP AI Core. You can also run LLaMa 3, Phi3, Mistral, Mixtral, LLaVa, and other [supported models in Ollama](https://ollama.com/library). \n",
    "\n",
    "### Prerequisites\n",
    "Before running this notebook, please assure you have performed the [Prerequisites](../../README.md) and [01-deployment.ipynb](01-deployment.ipynb). As a result, a deployment of Ollama scenario is running in SAP AI Core. <br/><br/>\n",
    "\n",
    "If the configuration and deployment are created through SAP AI Launchpad, please manually update the configuration_id and deployment_id in [env.json](env.json)\n",
    "```json\n",
    "{\n",
    "    \"configuration_id\": \"<YOUR_CONFIGURATION_ID_OF_OLLAMA_SCENARIO>\",\n",
    "    \"deployment_id\": \"<YOUR_DEPLOYMENT_ID_BASED_ON_CONFIG_ABOVE>\"\n",
    "}\n",
    "```\n",
    " \n",
    "### The high-level flow:\n",
    "- Load configurations info\n",
    "- Connect to SAP AI Core via SDK\n",
    "- Check the status and logs of the deployment\n",
    "- Pull model from ollama model repository through API\n",
    "- Inference the model with OpenAI-compatible chat completion API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55bd7b",
   "metadata": {},
   "source": [
    "#### 1.Load config info \n",
    "- resource_group loaded from [config.json](../config.json)\n",
    "- deployment_id(created in 01-deployment.ipynb) loaded [env.json](env.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90f1e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from ai_api_client_sdk.ai_api_v2_client import AIAPIV2Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5eee26b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment id:  df240ccfb2d899b0  resource group:  oss-llm\n"
     ]
    }
   ],
   "source": [
    "# Please replace the configurations below.\n",
    "# config_id: The target configuration to create the deployment. Please create the configuration first.\n",
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "with open(\"./env.json\") as f:\n",
    "    env = json.load(f)\n",
    "\n",
    "deployment_id = env[\"deployment_id\"]\n",
    "resource_group = config.get(\"resource_group\", \"default\")\n",
    "print(\"deployment id: \", deployment_id, \" resource group: \", resource_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd694c3",
   "metadata": {},
   "source": [
    "#### 2.Initiate connection to SAP AI Core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a4cc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aic_sk = config[\"ai_core_service_key\"]\n",
    "base_url = aic_sk[\"serviceurls\"][\"AI_API_URL\"] + \"/v2/lm\"\n",
    "ai_api_client = AIAPIV2Client(\n",
    "    base_url= base_url,\n",
    "    auth_url=aic_sk[\"url\"] + \"/oauth/token\",\n",
    "    client_id=aic_sk['clientid'],\n",
    "    client_secret=aic_sk['clientsecret'],\n",
    "    resource_group=resource_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ffb297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = ai_api_client.rest_client.get_token()\n",
    "headers = {\n",
    "        \"Authorization\": token,\n",
    "        'ai-resource-group': resource_group,\n",
    "        \"Content-Type\": \"application/json\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7b416",
   "metadata": {},
   "source": [
    "#### 3.Check the deployment status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d46cf76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment-df240ccfb2d899b0 is running. Ready for inference request\n"
     ]
    }
   ],
   "source": [
    "# Check deployment status before inference request\n",
    "deployment_url = f\"{base_url}/deployments/{deployment_id}\"\n",
    "response = requests.get(url=deployment_url, headers=headers)\n",
    "resp = response.json()    \n",
    "status = resp['status']\n",
    "\n",
    "deployment_log_url = f\"{base_url}/deployments/{deployment_id}/logs\"\n",
    "if status == \"RUNNING\":\n",
    "        print(f\"Deployment-{deployment_id} is running. Ready for inference request\")\n",
    "else:\n",
    "        print(f\"Deployment-{deployment_id} status: {status}. Not yet ready for inference request\")\n",
    "        #retrieve deployment logs\n",
    "        #{{apiurl}}/v2/lm/deployments/{{deploymentid}}/logs.\n",
    "\n",
    "        response = requests.get(deployment_log_url, headers=headers)\n",
    "        print('Deployment Logs:\\n', response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b4fcb",
   "metadata": {},
   "source": [
    "#### 4.Pull the model into Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d86047d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"phi3:14b\"\n",
    "#model = \"llama3:8b\"\n",
    "#model = \"mistral:latest\"\n",
    "#model = \"phi3:latest\" \n",
    "#model = \"mistral:7b-instruct-q5_K_M\"\n",
    "#model = \"mixtral:8x7b-instruct-v0.1-q4_0\" #Important: please resource plan to infer.l in byom-oss-llm-templates/ollama-template.yaml\n",
    "\n",
    "deployment = ai_api_client.deployment.get(deployment_id)\n",
    "inference_base_url = f\"{deployment.deployment_url}/v1\"\n",
    "openai_base_url = deployment.deployment_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc07e607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/df240ccfb2d899b0/v1/api/pull\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#let's pull the mistral model from ollama\u001b[39;00m\n\u001b[1;32m      6\u001b[0m json_data \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: model}\n\u001b[0;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResult:\u001b[39m\u001b[38;5;124m'\u001b[39m, response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/WorkSpace/OneDrive/12-Technology/byom-oss-llm-ai-core/oss-llm-env/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/WorkSpace/OneDrive/12-Technology/byom-oss-llm-ai-core/oss-llm-env/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/WorkSpace/OneDrive/12-Technology/byom-oss-llm-ai-core/oss-llm-env/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/WorkSpace/OneDrive/12-Technology/byom-oss-llm-ai-core/oss-llm-env/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/WorkSpace/OneDrive/12-Technology/byom-oss-llm-ai-core/oss-llm-env/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/WorkSpace/OneDrive/12-Technology/byom-oss-llm-ai-core/oss-llm-env/lib/python3.11/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/WorkSpace/OneDrive/12-Technology/byom-oss-llm-ai-core/oss-llm-env/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/WorkSpace/OneDrive/12-Technology/byom-oss-llm-ai-core/oss-llm-env/lib/python3.11/site-packages/urllib3/connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/WorkSpace/OneDrive/12-Technology/byom-oss-llm-ai-core/oss-llm-env/lib/python3.11/site-packages/urllib3/connection.py:653\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[1;32m    651\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 653\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[1;32m    673\u001b[0m \u001b[38;5;66;03m# Forwarding proxies can never have a verified target since\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# the proxy is the one doing the verification. Should instead\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# use a CONNECT tunnel in order to verify the target.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;66;03m# See: https://github.com/urllib3/urllib3/issues/3267.\u001b[39;00m\n",
      "File \u001b[0;32m~/WorkSpace/OneDrive/12-Technology/byom-oss-llm-ai-core/oss-llm-env/lib/python3.11/site-packages/urllib3/connection.py:806\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[1;32m    804\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 806\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[0;32m~/WorkSpace/OneDrive/12-Technology/byom-oss-llm-ai-core/oss-llm-env/lib/python3.11/site-packages/urllib3/util/ssl_.py:465\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:  \u001b[38;5;66;03m# Defensive: in CI, we always have set_alpn_protocols\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m~/WorkSpace/OneDrive/12-Technology/byom-oss-llm-ai-core/oss-llm-env/lib/python3.11/site-packages/urllib3/util/ssl_.py:509\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    506\u001b[0m     SSLTransport\u001b[38;5;241m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1108\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1106\u001b[0m             \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1108\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1379\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pull the model from ollama model repository\n",
    "endpoint = f\"{inference_base_url}/api/pull\"\n",
    "print(endpoint)\n",
    "\n",
    "#let's pull the mistral model from ollama\n",
    "json_data = {  \"name\": model}\n",
    "\n",
    "response = requests.post(endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa026a",
   "metadata": {},
   "source": [
    "Next, let's list the model and check if the target model is listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ff40e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/df240ccfb2d899b0/v1/api/tags\n",
      "Result: {\"models\":[{\"name\":\"phi3:14b\",\"model\":\"phi3:14b\",\"modified_at\":\"2024-05-23T09:50:06.013426087Z\",\"size\":7897126241,\"digest\":\"1e67dff39209b792d22a20f30ebabe679c64db83de91544693c4915b57e475aa\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"phi3\",\"families\":[\"phi3\"],\"parameter_size\":\"14.0B\",\"quantization_level\":\"F16\"},\"expires_at\":\"0001-01-01T00:00:00Z\"},{\"name\":\"llama3:latest\",\"model\":\"llama3:latest\",\"modified_at\":\"2024-05-23T02:47:43.160643007Z\",\"size\":4661224676,\"digest\":\"365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"8.0B\",\"quantization_level\":\"Q4_0\"},\"expires_at\":\"0001-01-01T00:00:00Z\"},{\"name\":\"llama3:8b\",\"model\":\"llama3:8b\",\"modified_at\":\"2024-05-23T02:19:25.163506171Z\",\"size\":4661224676,\"digest\":\"365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"8.0B\",\"quantization_level\":\"Q4_0\"},\"expires_at\":\"0001-01-01T00:00:00Z\"}]}\n"
     ]
    }
   ],
   "source": [
    "# Check the model list \n",
    "endpoint = f\"{inference_base_url}/api/tags\"\n",
    "print(endpoint)\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7d13c",
   "metadata": {},
   "source": [
    "#### 5.Inference completion and chat completion APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0658246",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_api_endpoint = f\"{inference_base_url}/api/generate\"\n",
    "chat_api_endpoint = f\"{inference_base_url}/api/chat\"\n",
    "openai_chat_api_endpoint = f\"{openai_base_url}/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848db40",
   "metadata": {},
   "source": [
    "##### 5.1 Sample#1: Test Ollama's Completion API\n",
    "Let's test it with a general Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eb1b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {\"model\":\"phi3:14b\",\"created_at\":\"2024-05-23T09:56:47.107018179Z\",\"response\":\"{\\n  \\\"Sunrise\\\": \\\"Pinkish to orange\\\",\\n  \\\"Morning\\\": \\\"Light blue\\\",\\n  \\\"Afternoon\\\": \\\"Blue\\\",\\n  \\\"Evening\\\": \\\"Orange and red hues, eventually transitioning to darker shades of blue or black at night\\\"\\n}\",\"done\":true,\"done_reason\":\"stop\",\"context\":[32010,13,5618,2927,338,278,14744,472,1422,3064,310,278,2462,29973,2538,2818,297,4663,32007,13,32001,13,29912,13,29871,376,29903,348,29878,895,1115,376,29925,682,728,304,24841,613,13,29871,376,28581,1076,1115,376,20769,7254,613,13,29871,376,29909,29888,16691,1115,376,21319,613,13,29871,376,29923,854,292,1115,376,29949,3881,322,2654,298,1041,29892,10201,9558,292,304,6501,261,528,3076,310,7254,470,4628,472,4646,29908,13,29913,32007],\"total_duration\":4143650900,\"load_duration\":1225039,\"prompt_eval_count\":20,\"prompt_eval_duration\":354871000,\"eval_count\":69,\"eval_duration\":3740907000}\n"
     ]
    }
   ],
   "source": [
    "#test ollama's completion api\n",
    "json_data = {\n",
    "  \"model\": model,\n",
    "  \"prompt\": \"What color is the sky at different times of the day? Respond in JSON\",\n",
    "  \"format\": \"json\", #JSON mode\n",
    "  \"stream\": False   #Streaming or not\n",
    "}\n",
    "\n",
    "response = requests.post(url=completion_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df95b2",
   "metadata": {},
   "source": [
    "##### 5.2 Test Ollama's Chat Completion API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c9f16",
   "metadata": {},
   "source": [
    "Now let's test Ollama's [OpenAI compatible API for Chat Completion](https://github.com/ollama/ollama/blob/main/docs/openai.md), which is the exact API interface of Chat Completion of GPT-3.5/4 in SAP Generative AI Hub. \n",
    "##### Sample#2: Write a haiku about Ollama in AI Core\n",
    "Let's test its chat completion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c824efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {\"model\":\"phi3:14b\",\"created_at\":\"2024-05-23T09:56:57.319052141Z\",\"message\":{\"role\":\"assistant\",\"content\":\"AI's swift strides on data plains,  \\n\\nOllama core's logic runs free,  \\n\\nWisdom flows like streams.\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":1647161706,\"load_duration\":1149212,\"prompt_eval_count\":29,\"prompt_eval_duration\":297175000,\"eval_count\":35,\"eval_duration\":1263617000}\n"
     ]
    }
   ],
   "source": [
    "#let's test ollama openai-compatible chat completion api by writing a haiku\n",
    "sys_msg = \"You are a helpful assistant.\"\n",
    "user_msg = \"Write a haiku about running Ollama in AI Core\"\n",
    "json_data = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_msg\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "    ],\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url=chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7aae2c",
   "metadata": {},
   "source": [
    "##### 5.3 Sample#3: Chain of Thought\n",
    "Now let's conduct a test on a basic sample of Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23a61fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {\"model\":\"phi3:14b\",\"created_at\":\"2024-05-23T09:57:15.688146874Z\",\"message\":{\"role\":\"assistant\",\"content\":\"Step 1: Roger starts with 5 tennis balls.\\nStep 2: He buys 2 more cans of tennis balls, and each can contains n tennis balls (in this case n=3).\\nStep 3: Calculate the total number of tennis balls in the new cans by multiplying the number of cans (2) with the number of tennis balls per can (3): 2 * 3 = 6 tennis balls.\\nStep 4: Add the initial number of tennis balls Roger had to the total number of tennis balls from the new cans: 5 + 6 = 11 tennis balls.\\n\\nSo, Roger now has a total of 11 tennis balls.\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":6079293933,\"load_duration\":2207541,\"prompt_eval_count\":50,\"prompt_eval_duration\":338494000,\"eval_count\":152,\"eval_duration\":5647370000}\n"
     ]
    }
   ],
   "source": [
    "sys_msg = \"You are a helpful assistant.\"\n",
    "user_msg = \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?Let's thinks step by step.\"\n",
    "\n",
    "json_data = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_msg\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "    ],\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url=chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b4ab1",
   "metadata": {},
   "source": [
    "##### 5.4 Sample#4: Customer Message Processing with OpenAI-compatible Chat Completion API\n",
    "In our sample [btp-industry-use-cases/04-customer-interaction-gpt4](https://github.com/SAP-samples/btp-industry-use-cases/tree/main/04-customer-interaction-gpt4),GPT-3.5/4 is used to process customer messages in customer interactions and output in json schema with plain prompting.\n",
    "- Summarize customer message into title and a short description\n",
    "- Analyze the sentiment of the customer message\n",
    "- Extract the entities from the customer message, such as customer, product, order no etc.\n",
    "\n",
    "Let's see if the same scenario could be achieved with mistral-7b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74cfc90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {\"id\":\"chatcmpl-518\",\"object\":\"chat.completion\",\"created\":1716559993,\"model\":\"phi3:14b\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"{\\n    \\\"sentiment\\\": \\\"Negative\\\",\\n    \\\"title\\\": \\\"Coffee Machine Issues and Warranty Repair\\\",\\n    \\\"summary\\\": \\\"User experiences issues with coffee machine pressure loss. Has been repaired under warranty but problems persist. Purchased at Harvey Norman.\\\",\\n    \\\"entities\\\": [\\n        {\\\"field\\\": \\\"customer_name\\\", \\\"value\\\": null},\\n        {\\\"field\\\": \\\"product_name\\\", \\\"value\\\": \\\"Coffee Machine\\\"},\\n        {\\\"field\\\": \\\"store_name\\\", \\\"value\\\": \\\"Harvey Norman\\\"}\\n    ]\\n}\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":575,\"completion_tokens\":133,\"total_tokens\":708}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test its openai-compatible chat completion api with to process customer message with\n",
    "# summarization, sentiment analysis and entities extraction and output as json\n",
    "sys_msg = '''\n",
    "You are an AI assistant to process the input text. Here are your tasks on the text.\n",
    "1.Apply Sentiment Analysis\n",
    "2.Generate a title less than 100 characters,and summarize the text into a short description less than 200 characters\n",
    "3.Extract the entities such as customer,product,order,delivery,invoice etc from the text Here is a preliminary list of the target entity fields and description. Please extract all the identifiable entities even not in the list below. Don't include any field with unknown value. \n",
    "-customer_no: alias customer number, customer id, account id, account number which could be used to identify a customer.\n",
    "-customer_name: customer name, account name\n",
    "-customer_phone: customer contact number. -product_no: product number, product id\n",
    "-product_name\n",
    "-order_no: sales order number, order id\n",
    "-order_date \n",
    "-delivery_no: delivery number, delivery id\n",
    "-delivery_date: delivery date, shipping date\n",
    "-invoice_no: alias invoice number, invoice id, receipt number, receipt id etc. which can be used to locate a invoice.\n",
    "-invoice_date: invoice date, purchase date\n",
    "-store_name\n",
    "-store_location\n",
    "etc.\n",
    "    \n",
    "For those fields not in list must follow the Snakecase name conversation like product_name, no space allow. \n",
    "\n",
    "Output expected in JSON format as below: \n",
    "{\\\"sentiment\\\":\\\"{{Positive/Neutral/Negative}}\\\",\\\"title\\\":\\\"{{The generated title based on the input text less than 100 characters}}\\\",\\\"summary\\\":\\\"{{The generated summary based on the input text less than 300 characters}}\\\",\\\"entities\\\":[{\\\"field\\\":\\\"{{the extracted fields such as product_name listed above}}\\\",\\\"value\\\":\\\"{{the extracted value of the field}}\\\"}]}\n",
    "'''\n",
    "\n",
    "user_msg = '''\n",
    "Input text: \n",
    "Everything was working fine one day I went to make a shot of coffee it stopped brewing after 3 seconds Then I tried the milk frother it stopped after 3 seconds again I took it back they fixed it under warranty but it’s happening again I don’t see this machine lasting more then 2 years to be honest I’m spewing I actually really like the machine It’s almost like it’s losing pressure somewhere, they wouldn’t tell my what the problem was when they fixed it.. Purchased at Harvey Norman for $1,349. \n",
    "Product is used: Several times a week\n",
    " \n",
    "JSON:\n",
    "'''\n",
    "\n",
    "json_data = { \n",
    "  \"model\": model,\n",
    "  \"response_format\": {\"type\": \"json_object\"}, #JSON mode\n",
    "  \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_msg\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "\n",
    "response = requests.post(url=openai_chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160fdeb1",
   "metadata": {},
   "source": [
    "##### 5.5 Sample#5: Citizen Reporting App for Public Administrations with OpenAI-compatible Chat Completion API\n",
    "In our sample [btp-generative-ai-hub-use-cases/01-social-media-citizen-reporting-genai-hub](https://github.com/SAP-samples/btp-generative-ai-hub-use-cases/tree/main/01-social-media-citizen-reporting-genai-hub-genai-hub), the use case is around a fictitious city called \"Sagenai City\" facing challenges in managing and tracking maintenance in public areas. The city wants to improve the way they handle reported issues from the citizens, by analyzing social media posts & making informed decisions and so effectively tracking & managing issues in public spaces. and output in json schema with plain prompting.\n",
    "- Category\n",
    "- Priority\n",
    "- Summary\n",
    "- Description\n",
    "- Address\n",
    "- Sentiment\n",
    "\n",
    "Let's see if the same scenario could be achieved with open-source llm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bcaea4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {\"id\":\"chatcmpl-575\",\"object\":\"chat.completion\",\"created\":1716559693,\"model\":\"phi3:14b\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"{\\n    \\\"address\\\": \\\"Oakwood Road\\\",\\n    \\\"category\\\": \\\"PUBLIC CLEANLINESS\\\",\\n    \\\"description\\\": \\\"Piles of rubb. litter scattered everywhere on Oakwood Rd, Sagenai. Local govt not cleaning despite taxes.\\\",\\n    \\\"location\\\": \\\"(51.57470453612761, 0.003792117지)\\\",\\n    \\\"priority\\\": \\\"4-Low\\\",\\n    \\\"sentiment\\\": \\\"NEUTRAL\\\",\\n    \\\"summary\\\": \\\"Oakwood Road littered and dirty\\\"\\n}\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":721,\"completion_tokens\":146,\"total_tokens\":867}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys_msg = \"You're a helpful assistant\"\n",
    "user_msg = '''\n",
    "Extract information from the following social media post: \n",
    "redditPostId: 198qqqm, \n",
    "author: jacobtan89, \n",
    "title: Dirty public area, \n",
    "message: The public area on Oakwood Road in Sagenai is in a disgraceful state with piles of rubbish and litter scattered everywhere. The author is frustrated with the local authorities for not maintaining cleanliness despite the taxes they pay. They hope for immediate action. #CleanUpYourAct #OakwoodRoadNightmare #DisgustingNeighborhood \n",
    "Coordinates:(51.57470453612761,0.003792117010085437), \n",
    "postingDate: 2024-01-17T07:13:48.000Z            \n",
    "\n",
    "The output should be a only json code snippet formatted in the following schema\n",
    "{\n",
    "    \"address\": string  // Extract the address where the issue has been noticed. Return the street only and omit the town or country. For example: Oakwood Road.\n",
    "    \"category\": string  // Identify if the social media reports a situation related to one of the following categories:             \n",
    "        1. PUBLIC CLEANLINESS: Dirty public areas, overflowing dustbins and littering. Bulky waste in common areas.  \n",
    "        2. ROADS & FOOTPATHS: Including covered linkways, signboards & streetlights. E.g. Pot holes, huge cracks, etc.\n",
    "        3. FACILITY & PARK MAINTENANCE: Fallen trees, overgrown grass, and maintenance of park lighting and facilities.\n",
    "        4. PESTS: Sighting of bees and hornets, potential mosquito breeding sites, and more.\n",
    "        5. DRAINS & SEWERS: Choked, overflowing, or damaged drains, bad sewage smells, flooding.   \n",
    "        Output the category name. If none of the categories fits, or in doubt, return OTHER - PLEASE CHECK.  \n",
    "    \"description\": string  // Summarize the issue that is being reported in not more that 300 characters and a neutral tone.\n",
    "    \"location\": string  // Extract the coordinates where the issue has been notices. The format should be: (51.57470453612761,0.003792117010085437).\n",
    "    \"priority\": string  // Identify the priority to be given to the reported issues:\n",
    "        4-Low : the issue does not pose any problem with public safety and does not necessarily need to be handled urgently. \n",
    "        3-Medium : the issue does not cause any immediate danger, but it has significant and negative impact on the daily life of people in the neighborhood.\n",
    "        2-High : the issue needs to be resolved quickly because it can potentially cause dangerous situations or disruptions. \n",
    "        1-Very High : the issue needs to be handled as soon as possible, as it is a matter of public safety. Return the priority level. If in doubt, return 3-Medium    \n",
    "    \"sentiment\": string  // Extract the sentiment of the post: \n",
    "        1. NEUTRAL: if the issue is reported politely\n",
    "        2. NEGATIVE: if the post shows irritation, impatience, annoyance\n",
    "        3. VERY NEGATIVE: the post expresses rage, hatred\n",
    "    \"summary\": string  // Summarize the issue that is being reported in 40 characters and a neutral tone.\n",
    "}\n",
    "\n",
    "JSON:\n",
    "'''\n",
    "\n",
    "json_data = {\n",
    "  \"model\": model,\n",
    "  \"response_format\": {\"type\": \"json_object\"}, #JSON mode\n",
    "  \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_msg\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "\n",
    "response = requests.post(url=openai_chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
