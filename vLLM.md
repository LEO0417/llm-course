[LLM Compressor is Here: Faster Inference with vLLM](https://neuralmagic.com/blog/llm-compressor-is-here-faster-inference-with-vllm/)

[How to deploy DeepSeek V2 Lite models using vLLM](https://www.53ai.com/news/finetuning/2024090663471.html)
